## 逻辑回归与广义线性回归

### 一、Logistic函数

假设在P个独立自变量$X_1, X2, ..., X_p$的作用下，记Y取1的概率为P=P(Y=1|X)，取0的概率是1-P，取1和取0的概率之比为$\frac{P}{1-P}$ ，称为事件的优势比（odds），对优势比取自然对数即得到Logistic变换：$logit(P) = ln(\frac{P}{1-p})$
$$
令logit(P)  = ln(\frac{P}{1-p}) = z，则有P = \frac{1}{1+e^{-z}} \qquad 称P为Logistic函数
$$
Logistic函数的取值范围是[0， 1]

![Logistic函数](E:/gaocn.github.io/img/md_imgs/Logistic函数.png)



### 二、Logistic回归（softmax回归）

针对因变量是分类变量，特别是二元分类变量，可以把因变量变为0-1的取值。通过特定的函数将因变量转换为连续变量，然后再利用线性回归的方法求解。Logistic回归是广义线性回归的一个特例。

Logistic回归是延续回归的思路用于解决分类问题。Logistic回归的特点：

- 因变量为分类变量；
- 将自变量映射到[0，1]区间上；
- Sigmoid函数 $\sigma(z) = \frac{1}{1 + e^{-z}}$

$$
\sigma(z) = \frac{1}{1 + e^{-z}}     \\
\downarrow \\
P\{Y=1\} =  \frac{1}{1 + e^{-z}} = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + ... + \beta_nX_N)}} \\
P\{Y=0\} =  1- \frac{1}{1 + e^{-z}} = \frac{e^{-(\beta_0 + \beta_1X_1 + ... + \beta_nX_N)}}{1 + e^{-(\beta_0 + \beta_1X_1 + ... + \beta_nX_N)}} \\
ln\frac{P}{1-P} = ln\frac{P\{Y=1\}}{\{Y=0\}} = \beta_0 + \beta_1X_1 + ... + \beta_nX_N（两边取自然对数）转换为线性回归
$$

当$X_1, X_2, ..., X_n$为0时，$\beta_0$就是优势比。Logistic回归可以看做还是概率的估计，实际上求出来的值是事件发生的可能性。通常用的决策函数为$y^* = 1, \, \textrm{if} \, P(y=1|x) > 0.5$，选择0.5作为阈值是一个一般的做法，实际应用时根据不同情况可以选择不同阈值，如果对正例的判别准确性要求高，可以选择阈值大一些，对正例的召回要求高，则可以选择阈值小一些。

### 三、Logistic建模步骤

1. 根据分析目的设置特征，并筛选特征；有些时候自变量很多，但不是每一个都有效，需要进行自变量筛选，精简模型。
2. 列出回归方程，估计回归系数；
3. 进行模型检验；
4. 模型应用；

```python
# -*- coding: utf-8 -*-
#范例：逻辑回归
from sklearn.linear_model import LogisticRegression as LR
from sklearn.linear_model import RandomizedLogisticRegression as RLR 
import pandas as pd

#参数初始化
filename = 'd:/data/bankloan.xls'
data = pd.read_excel(filename)
#
# 因变量：违约
# 以后用户贷款时，根据用户违约情况，确定其是否为其贷款
#
#   年龄  教育  工龄  地址   收入 负债率   信用卡负债   其他负债  违约
#0  41     3    17    12   176    9.3     11.359392  5.008608   1
#1  27     1    10    6    31     17.3    1.362202   4.000798   0
#2  40     1    15    14   55     5.5     0.856075   2.168925   0
#3  41     1    15    14   120    2.9     2.658720   0.821280   0
#4  24     2     2    0    28     17.3    1.787436   3.056564   1
x = data.iloc[:,:8].as_matrix()
y = data.iloc[:,8].as_matrix()

#1. 特征筛选
#
#由于上述自变量很多，有一些可能是没用的，因此需要筛选
#
#通过随机逻辑回归模型筛选特征
rlr = RLR() 
rlr.fit(x, y) 

#获取特征筛选结果，true为选取，False尾部选取，默认大于0.25的会被选取
rlr.get_support() 
# array([False, False,  True,  True, False,  True,  True, False], dtype=bool)

# 通过scores_方法获取各个特征的分数
rlr.scores_
# array([ 0.115,  0.065,  0.98 ,  0.39 ,  0.   ,  0.995,  0.605,  0.04 ])

print(u'有效特征为：%s' % ','.join(data.columns[rlr.get_support()]))
# 有效特征为：工龄,地址,负债率,信用卡负债

x = data[data.columns[rlr.get_support()]].as_matrix() #筛选好特征
#  工龄  地址 负债率 信用卡负债
#0  17   12   9.3   11.359392
#1  10   6   17.3   1.362202
#2  15   14   5.5   0.856075
#3  15   14   2.9   2.658720
#4   2   0   17.3   1.787436

#2. 建立Logistic逻辑回归模型
lr = LR() 
lr.fit(x, y) #用筛选后的特征数据来训练模型

#给出模型的平均正确率，本例为81.4%
print(u'模型的平均正确率为：%s' % lr.score(x, y)) 
```

### 四、广义线性模型（GLM）

广义线性回归：只要对自变量或因变量进行变换，就能将其转换为线性回归模型，把自变量的线性预测函数当作因变量的估计值。把这样的非线性模型称为广义线性模型，与线性回归模型相比有如下特点：

- 随机误差项不一定服从正态分布，可以服从二项、泊松、负二项、正态、伽马、逆高斯等分布，这些分布被统称为指数分布族。
- 因变量和自变量通过联接函数产生影响，联接函数满足单调，可导。常用的联接函数：log、根号、ln等。



http://blog.csdn.net/ACdreamers/article/details/44663305

http://blog.csdn.net/acdreamers/article/details/44663091







#### 4.1 范例：广义线性模型

```python
import matplotlib.pyplot as plt
from sklearn import metrics
import numpy as np
import pandas as pd 
#限售额x与流通率y的关系
x=pd.DataFrame([1.5,2.8,4.5,7.5,10.5,13.5,15.1,16.5,19.5,22.5,24.5,26.5])
y=pd.DataFrame([7.0,5.5,4.6,3.6,2.9,2.7,2.5,2.4,2.2,2.1,1.9,1.8])
plt.scatter(x,y)
plt.show()
```

![GLM](E:/gaocn.github.io/img/md_imgs/GLM.png)

**方法1：直接采用直线回归拟合$Y = a + bX$**

```python
from sklearn.linear_model import LinearRegression
linreg = LinearRegression()
linreg.fit(x,y)

# 系数b的取值 linreg.coef_
print('Coefficients: \n', linreg.coef_)
# Coefficients: [[-0.17002988]]

y_pred = linreg.predict(x)
# 平均平方误差
print("MSE:",metrics.mean_squared_error(y,y_pred))
# MSE: 0.4942186278  (误差太大)

#方差得分: 1 is perfect prediction
print('Variance score: %.2f' % linreg.score(x, y))
# Variance score: 0.80
```

**方法2：多项式回归，假设使用二次多项式$Y=a+bX+cX^2$**

```python
x1=x
x2=x**2
x1['x2']=x2

linreg = LinearRegression()
linreg.fit(x1,y)

# The coefficients
print('Coefficients: \n', linreg.coef_)
# Coefficients: [[-0.4656313   0.01075704]]

y_pred = linreg.predict(x)
# 平均平方误差
print("MSE:",metrics.mean_squared_error(y,y_pred))
# MSE: 0.118119570951
```

**方法3：对数回归 $Y = a + blogX$**

```python
x2=pd.DataFrame(np.log(x[0]))

linreg = LinearRegression()
linreg.fit(x2,y)

print('Coefficients: \n', linreg.coef_)
# Coefficients: [[-1.75683848]]

y_pred = linreg.predict(x2)
print("MSE:",metrics.mean_squared_error(y,y_pred))
# MSE: 0.0355123571858
```

**方法4：指数回归 $Y=ae^{bX} $**

预处理，对指数两边同时取log可以得到：$logY = loga + bX$

```python
y2=pd.DataFrame(np.log(y))

linreg = LinearRegression()
linreg.fit(pd.DataFrame(x[0]),y2)

print('Coefficients: \n', linreg.coef_)
# Coefficients: [[-0.04880874]]
y_pred = linreg.predict(pd.DataFrame(x[0]))
print("MSE:",metrics.mean_squared_error(y2,y_pred))
# MSE: 0.0147484198861
```

**方法5：幂函数回归 $Y = aX^b$**

预处理，两边取对数可以得到：$logY = loga + blogX$

```python
x2=pd.DataFrame(np.log(x[0]))
y2=pd.DataFrame(np.log(y))

linreg = LinearRegression()
linreg.fit(x2,y2)

print('Coefficients: \n', linreg.coef_)
# Coefficients: [[-0.47242789]]
y_pred = linreg.predict(x2)

print ("MSE:",metrics.mean_squared_error(y2,y_pred))
# MSE: 0.00108621015916

print('Variance score: %.2f' % linreg.score(x2, y2))
# Variance score: 0.99  方差得分是最高的，因此可以判断x与y的关系为幂函数关系
```

