#无监督聚类

##一、聚类算法-KMeans



KMeans算法流程

1. 随机初始化n_cluster个点作为簇的中心点；
2. 对于其他所有每一个点a,计算其到这n_cluster个中心点得距离，将a归为其中距离最小的那一个中心所在的类别；
3. 对第2步得到的n_cluster个簇，重新计算这n_cluster个簇的中心点；
4. 转到2步，直到无论怎么更新中心点每个簇中的元素不在发生变化，就停止迭代；





举例**

无监督问题通常使用距离判断两个样本的相似度，例如利用欧式距离判断样本相似度。

数据：美国国会议员投票数据

```python
import padas as pd

# 美国国会议员投票数据
votes = pd.read_csv('114_congress.csv')

# 计算欧式距离
from sklearn.metrics.pairwise import euclidean_distances
X = [[0, 1], [1, 1]]
euclidean_distances(X, X)
# array([[ 0.,  1.],
#       [ 1.,  0.]])
euclidean_distances(X, [[0, 0]])
#    array([[ 1.        ],
#           [ 1.41421356]])

from sklearn.cluster import KMeans

kmeans_models = KMeans(n_clusters=2, random_state=2)
senator_distances = kmeans_models.fit_transform(votes.iloc[:, 3:])
# 打印数据为N * n_cluster维数据
print(senator_distance)
# 第k行表示第k个样本距离第一个cluster的中心点的距离、距离第二个cluster中心点的距离
#[[6.78345, 18.23673434]
# [5.23981, 12.48612469]
# ....
# ]

labels = kmeans_models.labels_
# crosstab表示
print(pd.crosstab(labels, votes['party']))
#party  D  I  R
#row_0 
# 0    41  2  0  预测结果为民主党的，有2个无党派
# 1    3   0  54 第二个簇越大多数为R，有3个是民主党投的结果与共和党一样

# 把上面3个归类为第二个簇的样本找出来
democratic_outliers = votes[(labels == 1)&(votes['party'] == 'D')]
print(democratic_outliers)

import matplotlib.pyplot as plt 
plt.scatter(x=senator_distances[:, 0], y=senator_distances[:, 1], c=labels)
plt.show()

#
# 聚簇问题中，通常需要找到一些离群点
# 将边缘点作为离群点
# 如何找离群点？要么距离聚簇1的中心距离比较大，要么距离聚簇2的中心距离比较大
# 下面分别是两个点距离聚簇1，2的距离
extremist = [3.4, .24]
moderate = [2.6, 2]
# 对距离指标求3次方和，可以看出这个值能反映样本点的离群程度
print(3.4 ** 3 + .24 ** 3)
print(2.6 ** 3 + 2 ** 3)
# 39.317824
# 25.576

# 对所有样本点三次方求和
extremist = (senator_distances ** 3).sum(axis=1)
votes['extremist'] = extremist
votes.sort_values("extremist", inplace=True, ascending=False)
print(votes.head())
```



###1.2案例：对NBA球员进行评估

```python
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
import math

nba = pd.read_csv('nba_2013.csv')

point_guards = nba[nba['pos']=='PG']
point_guards['ppg'] = point_guards['pts'] / point_guards['g']
point_guards[['pts', 'g', 'ppg']].head()
#  pts  g   ppg (总得分，打球场次，平均每场得分)
#  930  71  13.098
#  150  20  7.5
#  660  79  8.354
#  666  73  6.35

point_guards = point_guards[point_guards['tov'] != 0]
# 助攻与失误比
point_guards['atr'] = point_guards['ast'] / point_guards['tov']

# 只使用两个特征： ppg、atr对球员分类
plt.scatter(point_guards['ppg'], point_guards['atr'], c='y')
plt.title('Point Guards')
plt.xlabel('Points Per Game')
plt.ylabel('Assist Turnover Ratio')
plt.show()

# 想把数据聚类成5，n_cluster=5
"""
KMeans算法流程
1. 随机初始化n_cluster个点作为簇的中心点；
2. 对于其他所有每一个点a,计算其到这n_cluster个中心点得距离，将a归为其中距离最小的那一个中心所在的类别；
3. 对第2步得到的n_cluster个簇，重新计算这n_cluster个簇的中心点；
4. 转到2步，直到无论怎么更新中心点每个簇中的元素不在发生变化，就停止迭代；
"""

n_cluster = 5
# 随机选取5个索引
random_initial_points = np.random.choice(point_guards.index, size=n_cluster)
# 随机初始化5个点作为簇的中心点
centroids = point_guards.loc[random_initial_points]

# 将初始化的点用红色标注
plt.scatter(point_guards['ppg'], point_guards['atr'], c='yellow')
plt.scatter(centroids['ppg'], centroids['atr'], c='red')
plt.show()

def centroids_to_dict(centroids):
    dictionary = dict()
    # iterating counter we use to generate a cluster_id
    counter = 0

    # iterate a pandas data frame row-wise using iterrows()
    for index, row in centroids.iterrows():
        coordinates = [row['ppg'], row['atr']]
        dictionary[counter] = coordinates
        counter += 1
    return dictionary
centroids_dict = centroids_to_dict

def calculate_distance(centroids, player_values):
    root_distance = 0

    for x in range(0, len(centroids)):
        difference = centroids[x] - player_values[x]
        squared_difference = difference ** 2
        root_distance += squared_difference
    eculid_distace = math.sqrt(root_distance)
    return eculid_distace

# 每一个点属于哪一个类别
def assign_to_cluster(row):
    lowest_distance = -1
    closest_clutser = -1

    for cluster_id, centroid in centroids_dict.items():
        df_row = [row['pps'], row['atr']]
        euclidean_distace = calculate_distance(centroid, df_row)

        if lowest_distance == -1:
            lowest_distance = euclidean_distace
            closest_clutser = cluster_id
        elif euclidean_distace < lowest_distance:
            lowest_distance = euclidean_distace
            closest_clutser = cluster_id
    return closest_clutser

point_guards['cluster'] = point_guards.apply(lambda row: assign_to_cluster(row), axis=1)

def visualize_clusters(df, num_clusters):
    colors = ['b','g','r','c','m','y','k']

    for n in range(num_clusters):
        clustered_df = df[df['cluster'] == n]
        plt.scatter(clustered_df['ppg'], clustered_df['atr'], c=colors[n-1])
        plt.xlabel('Points Per Game', fontsize=12)
        plt.ylabel('Assist Turnover Radio', fontsize=12)
    plt.show()

visualize_clusters(point_guards, 5)

def recalculate_centroids(df):
    new_centroids_dict = dict()

    for cluster_id in range(0, num_clusters):
        pass
    
    return new_centroids_dict
# 循环进行如下操作指导
centroids_dict = recalculate_centroids(point_guards)
point_guards['cluster'] = point_guards.apply(lambda row: assign_to_cluster(row), axis=1)


# sklearn实现
from sklearn.cluster import KMeans
kmeans = KMeans(n_cluster=5)
kmeans.fit(point_guards[['ppg','atr']])
point_guards['cluster'] = kmeans.labels_

visualize_clusters(point_guards, 5)
```

###1.3案例：Kmeans进行图像压缩

```python

from sklearn.cluster import KMeans
from skimage import io
import numpy as np 

# 对像素点进行聚类，将像素点由256种压缩到128种
# 1. 彩色图压缩为灰度图
# 2. 将像素的取值范围进行压缩

image = io.imread('G:\\data\\1.JPG')
# io.imshow(image)
# io.show()

rows = image.shape[0]
cols = image.shape[1]

image = image.reshape((image.shape[0] * image.shape[1], 3))

#对RGB三个通道进行聚类，原来有0-255个取值，这里压缩为128个
kmeans = KMeans(n_clusters=128, n_init=10, max_iter=200)
kmeans.fit(image)

clusters = np.array(kmeans.cluster_centers_, dtype=np.uint8)
labels = np.array(kmeans.labels_, dtype=np.uint8)
labels = labels.reshape(rows, cols)
print(clusters.shape)
np.save('G:\\data\\comressed_1.npy', clusters)
io.imsave('G:\\data\\compressd_1.jpg', labels)
```



##二、聚类算法-DBSCAN





##三、聚类算法-KNN(K近邻)

如下图所示，根据K值不同得到不同的结果

- 如果k=3，绿色远点的最近的3个邻居是2个红色的小三角和1个蓝色的小正方形，少数服从多数，基于统计的方法，判定绿色的这个待分类点属于红色的三角形一类。
- 如果k=5，绿色圆点的最近的5个邻居是2个红色小三角和3个蓝色正方形，还是少数服从多数，基于统计的方法，哦安定绿色的这个待分类点属于蓝色的正方形一类。

![-近邻举](../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/imgs_md/K-%E8%BF%91%E9%82%BB%E4%B8%BE%E4%BE%8B.png)

**K-近邻算法描述**

1. 计算已知类别数据集中的点与当前未知类别属性数据集中的点的距离(欧式距离)；
2. 按照距离依次排序；
3. 选取与当前点距离最小的K个点；
4. 确定前K个点所在类别的出现频率；
5. 返回前K个点出现频率最高的类别作为当前点预测分类；

KNN算法本身简单有效，它是一种lazy-learning算法，分类器不需要使用训练集进行训练，训练时间复杂度为0。KNN分类的计算复杂度和训练集中的文档数目成正比，即如果训练集中文档总数为N，那么KNN的分类时间复杂度为O(N)。

K值得选择，距离度量和分类决策规则是该算法的三个基本要素。

**问题**：该算法在分类时有个主要的不足是，当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本上时，该样本的K个邻居中大容量样本占多数？解决办法是**不同的样本给予不同权重**。

```python
import numpy as np
class NearestNeighor:
    def __init__(self):
        pass
    def trai(self, X, y):
        """
        X is N*D where each row is an example. Y is 1-dimesion of size N
        """
        self.Xtr = X
        self.Ytr = y
    def predict(self, X):
        """
        X is N*D where each row is an example we wish to predict label for
        """
        num_test = X.shape[0]
        # make sure that the output type matches the input type
        Ypred = np.zeros(num_test, dtype=self.dtype)
        
        for i in range(num_test):
            # find the nearest trainging image to the i'th test image
            # usig the L1 distance(sum of absolute value different)
            distaces = np.sum(np.abs(self.Xtr - X[i, :]), axis=1)
            min_index = np.argmin(distaces)
            Ypred[i] = self.Ytr[min_index]
        return Ypred
```













