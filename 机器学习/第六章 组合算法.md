#第六章 组合算法

提升分类器准确率的组合方法包括：Bagging、Boosting和随机森林。基于学习数据集抽样产生若干训练集，使用训练集产生若干分类器，每个分类器分别迚行预测，通过简单选举多数，判定最终所属分类。

![组合算法](../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/imgs_md/%E7%BB%84%E5%90%88%E7%AE%97%E6%B3%95.png)

为什么组合方法能够提高分类准确率[^6]？当分类器很少的时候，一般来说分类平面通常就是一个简单的几何形状，例如：一条直线、一个平面，线性分类器和Bayes分类器本质上就是一个分离平面；而当用组合方法将若干个分类器组合在一起的时候，就相当于有很多条直线构成分类边界，这样就构成了一个折线，分类器越多，这条折线的形状越复杂，当多到一定程度时就变成了分离曲线，如下图右子图所示，这条曲线对样本的分离肯定比折线更好，特别是在边界上有犬牙交错的不同类别的样本时，分离曲线更有效。

![组合算法提高分类准确率](../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/imgs_md/%E7%BB%84%E5%90%88%E7%AE%97%E6%B3%95%E6%8F%90%E9%AB%98%E5%88%86%E7%B1%BB%E5%87%86%E7%A1%AE%E7%8E%87.png)

##6.1 Bagging(Bootstrap Aggregation)算法

装袋算法为学习方案创建组合分类模型，其中每个模型给出等权重预测。**组合算法中需要解决的核心问题**：

- 从原始数据集D中，通过抽样产生k个子学习集，如何抽样，抽样方法是什么？
- 数据被抽样出来以后形成了新的学习集，新的学习集怎么样把分类器训练出来，训练的方法是什么？

Bagging算法通过**有放回抽样**从原始数据集D中抽取k个子学习集，目的是让前后两次抽样在概率上相互条件独立的，即前后抽取不会相互影响。Bagging算法**优点**：1. 准确率明显高于组合中任何单个的分类器；2. 对于较大的噪音，表现丌至于很差，并且具有鲁棒性；3. 不容易过度拟合。

**自助样本（Bootstrap）**：是有放回抽样方法，抽样的个数与原始数据集是一样的。例如：原始学习集中有1000条数据，那么就采用有放回抽样方法抽样1000次，得到1000条数据形成一个新的学习集，因为是有放回抽样，因此新学习集中的数据有可能会有重复的样本，而原始学习集中的样本有的可能没有抽到，根据学者统计采用自助样本方法会抽取到60%多的样本。

**装袋算法流程**

`输入`

- D：d个训练元组的集合；
- k：组合分类器中的模型数；
- 一种学习方案，例如：决策树算法、后向传播等；

`输出`：组合分类器（复杂模型$M^*$）

1. 创建k个模型；
2. 通过对D*有放回抽样* ，创建自主样本$D_i$；
3. 使用$D_i$和学习方案导出模型$M_i$；
4. 使用组合分类器对元组$X$分类，让k个模型都对$X$分类并返回多数表决；








##6.2 Boosting算法

训练集中的元组被分配权重，权重影响抽样，权重越大，越可能被抽取，迭代训练若干个分类器，在前一个分类器中被错误分类的元组，会被提高权重，使到它在后面建立的分类器里被更加“关注” ，最后分类也是由所有分类器一起投票，投票权重取决于分类器的准确率。Boosting算法优点是准确率较Bagging更高，但容易出现过度拟合。

**Adaboost（Adaptive Boosting）算法 **

Adaboost是一种提升算法，创建分类器的组合，每次给出一个加权投票。

`输入`

- D：训练元组集；
- k：轮数，每轮产生一个分类器；
- 一种学习方案，例如：决策树算法、后向传播等；

`输出`：组合分类器（复杂模型$M^*$）

`训练方法`

1. 将D中每个元组的权重初始化为$\frac{1}{d}$，令$i \leftarrow 1 \quad to \quad k$；
2. 根据元组的权重从D中**有放回抽样**，得到$D_i$
3. 使用训练集$D_i$到处模型$M_i$；
4. 计算$M_i$的误差率$error(M_i)$；
5. 若$error(M_i) \gt 0.5$，则转到2步重试；
6. 对于$D_i$中每个被正确分类的元组，更新元组权重，是正确分类的元组权重降低，而错误分类的元组权重不变，更新公式为 $正确分类的元组权重 * error(M_i)/(1 - error(M_i))$；
7. 规范化每个元组的权重，转到步骤2；

`预测方法`：使用组合分类器对元组$X$分类

1. 将每个类的权重初始化为0；
2. 令$i \leftarrow 1 \quad to \quad k$，对于每一个没类器$M_i$；
3. 计算分类器的投票权重$w_i  = log(\frac{1-  error(M_i)}{error(M_i)})$，准确率高的分类器权重越高；
4. 从分类器$M_i$计算X的类预测：$c = M_i(X)$；
5. 将$w_i$加到类$c$的权重，转到步骤2；
6. 返回具有最大权重的类






****

##参考文献

[^1]: Friedman J H. Greedy function approximation: a gradient boosting machine[J]. Annals of statistics, 2001: 1189-1232.
[^2]: Friedman J H. Stochastic gradient boosting[J]. Computational Statistics & Data Analysis, 2002, 38(4): 367-378.

