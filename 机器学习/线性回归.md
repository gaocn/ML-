##线性回归[^1]


$$
h_{\theta}(x) = \theta_0 +\theta_1 x_1 + ... + \theta_n x_n = \sum_{i=0}^{n}\theta_i x_i = \theta^T x，其中x_0=1 \\

预测值与真实值得关系：
y^{{i}} = \theta^T x^{(i)} + \epsilon^{(i)}
$$
假设误差$\epsilon^{(i)}$ 是独立同分布，通常认为服从均值为0方差为$\sigma^2$ 的高斯分布，则有
$$
\epsilon^{(i)} = y^{(i)} - \theta^T x^{(i)} \\
p(\epsilon^{(i)}) = \frac{1}{\sqrt{2 \pi} \sigma} exp(- \frac{(\epsilon^{(i)})^2}{2 \sigma^2}) \\

p(y^{(i)} | x^{(i)}； \theta) = \frac{1}{\sqrt{2 \pi} \sigma} exp(- \frac{(y^{(i)} - \theta^T x^{(i)} )^2}{2 \sigma^2})
$$
$p(y^{(i)} | x^{(i)}； \theta) $中$\theta$ 为要求出的参数值，表示需要找到$\theta$ 值与$x^{(i)}$ 的组合后得到的预测值接近$y^{(i)}$ 的概率，概率值越大$\theta$ 参数越好。如何求出$\theta$ ?利用最大似然函数求出，对于所有样本来说，当概率值最大值时的$\theta$ 求法为
$$
L(\theta)  = \prod_{i=1}^m p(y^{(i)} | x^{(i)}； \theta) =  \prod_{i=1}^m  \frac{1}{\sqrt{2 \pi} \sigma} exp(- \frac{(y^{(i)} - \theta^T x^{(i)} )^2}{2 \sigma^2}) \\
l(\theta) = logL(\theta) = \sum_{i=1}^{m} log[\frac{1}{\sqrt{2 \pi} \sigma} exp(- \frac{(y^{(i)} - \theta^T x^{(i)} )^2}{2 \sigma^2}) ] \\
= m \cdot log\frac{1}{\sqrt{2 \pi} \sigma} - \frac{1}{\sigma^2} \cdot \frac{1}{2}\sum_{i=1}^{m}(y^{(i)} - \theta^T x^{(i)})^2 \\
若要使l(\theta)最大，则上式后半部分的越小越好 \\
目标函数：J(\theta) = \frac{1}{2} \sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})^2
$$

$$
J(\theta) = \frac{1}{2} \sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})^2 = \frac{1}{2}(x\theta  - y)^T( x \theta - y) \\
\triangledown J(\theta) = \triangledown [\frac{1}{2}(\theta^T x^T - y^T)( x \theta - y)] 
= \triangledown [\theta^T x^T x \theta - \theta^T x^T y - y^T x \theta + y^Ty] 
\\=\frac{1}{2}[2x^Tx\theta - x^Ty - (y^Tx)^T] = x^Tx\theta - x^Ty = 0 （矩阵求导）\\
\theta = (x^Tx)^{-1} x^T y
$$

![线性回归举例](imgs_md/线性回归举例.png)

```python
import matplotlib.pyplot as plt
import numpy as np 
from sklearn import datasets

class LinearRegression(object):
    def __init__(self):
        self.theta = None
    
    def fit(self, X, y):
        print(X.shape)
        # Insert constant ones for bias weights
        # 第0列插入x.shape[0]行值为1
        X = np.insert(X, 0, 1, axis=1)
        print(X.shape)

        X_ = np.linalg.inv(X.T.dot(X))
        self.theta = X_.dot(X.T).dot(y)

    def predict(self, X):
        # Insert constant ones for bias weights
        X = np.insert(X, 0, 1, axis=1)
        y_pred = X.dot(self.theta)
        return y_pred
        
def mean_squared_error(y_true, y_pred):
    mse = np.mean(np.power(y_true - y_pred, 2))
    return mse

def main():
    # 加载数据集
    diabetes = datasets.load_diabetes()

    # 422 * 1，样本数422个，特征为1个 shape=(422, 1)
    X = diabetes.data[:, np.newaxis, 2]
    print(X.shape)

    # 划分训练集、测试集
    x_train, x_test = X[:-20], X[-20:]
    y_train, y_test = diabetes.target[:-20] , diabetes.target[-20:]

    clf = LinearRegression()
    clf.fit(x_train, y_train)
    y_pred = clf.predict(x_test)

    # 打印平均平方差误差(Mean Squred Error)
    print('Mean Squared Error: ', mean_squared_error(y_test, y_pred))
    plt.scatter(x_test[:, 0], y_test, color='black')
    plt.plot(x_test[:, 0], y_pred, color='blue', linewidth=3)
    plt.show()
```



##逻辑（Logistic）回归和梯度下降[^2]

逻辑回归实际上做了一个分类的事，通常逻辑回归适用于做二元分类。



S型函数
$$
g(z) = \frac{1}{1 + e^{-z}} \\
g'(x) = \frac{e^{-x}}{(1 + e^{-x})^2} = \frac{1}{1 + e^{-x}}  \cdot \frac{e^{-x}}{1 + e^{-x}} \\ =  \frac{1}{1 + e^{-x}}  \cdot (1-  \frac{1}{1 + e^{-x}}) = g(x)(1 - g(x))
$$
![sigmoid函数](imgs_md/sigmoid函数.png)

sigmoid函数能够将任何一个实数映射要[0,1]区间上，看做成一个概率值。

逻辑斯特回归实际上是将预测值$\theta^Tx$ 作为输入通过Sigmoid函数转换为概率值，实际上是将预测值用做分类任务。

$$
h_{\theta}(x) = g(\theta^T x) = \frac{1}{1 + e^{-\theta^Tx}}
$$

逻辑回归用于进行二分类
$$
P(y=1|x;\theta) = h_{\theta}(x) \\
P(y=0|x;\theta) = 1 - h_{\theta}(x) \\
p(y|x;\theta) = (h_{\theta}(x))^y(1- h_{\theta}(x))^{(1-y)} 求x,\theta的组合使得y值最接近 \\
L(\theta) = p(y|x:\theta) = \prod_{i=1}^{m}p(y^{(i)}|x^{(i)};\theta) =\prod_{i=1}^{m} (h_{\theta}(x^{(i)}))^{y^{(i)}}(1- h_{\theta}(x^{(i)}))^{(1-y^{(i)})} \\
l(\theta) = logL(\theta) = \sum_{i=1}^{m} y^{(i)}logh(x^{(i)}) + (1 - y^{(i)}) log(1- h(x^{(i)}))
$$
但是此时通过求导等于零食得到不到对应$\theta$ 的解的，因此可以通过==梯度下降==方法 



## 案例：信用卡欺诈













###混淆矩阵

**模型评估标准：召回率（recall）**

| Prediction  | Observation           |                       |
| ----------- | --------------------- | --------------------- |
|             | Admitted(1)           | Rejected(0)           |
| Admitted(1) | True Positive (TP)，  | False Positive (FP)， |
| Rejected(0) | False Negative (FN)， | True Negative (TN)，  |

假如某个班级有男生80人，女生20人，共计100人。目标是找出所有女生。现在某人挑出50人，其中20人是女生，另外还错误的把30个男生也当作女生挑选出来了。

|            | 相关，正类                           | 无关，负类                           |
| ---------- | ------------------------------------ | ------------------------------------ |
| 被检索到   | TP，正类判定为正类，确实是女生       | FP，负类判定为正类，男生被判定为女生 |
| 未被检索到 | FN，正类判定为负类，女生被判定为男生 | TN，负类判定为负类，男生被判定为男生 |

通过上述表格，可以清楚的得到：TP=20，FP=30，FN=0，TN=50

**True Positive Rate(Sensitivity)**

TPR指标衡量模型检测正例的效果，例如用模型检测病人是否患癌症，TP表示患病的人被正确的检测出来了，而FN则是患病的人被认为是正常的，这时候结果就严重的，在这个问题上需要考虑TPR，否则好多人会因为这个模型而受难。
$$
TPR =  \frac{True  \ Positives}{True \  Positives + False \  Nagetives}
$$
**True Negative Rate**

TNR指标衡量模型检测负例例的效果，例如用模型检测病人是否患癌症，TP是没患癌症的人被正确的检测出来了，TN是没患病的人被检测出来患有癌症。
$$
TNR = \frac{True \ Negative}{False \ Positive + True \ Negative}
$$
为什么说精度经常是不准确的、有欺骗性？例如有100个样本，其中有90个样本是属于1这个类别，而10个样本属于0这个类别，让分类器预测，假设模型全部预测为1类别，则精度也有90%，因此精度在样本非常不平衡的条件下是非常不准确的。很多情况下需要衡量TPR、TNR指标。

![样本的混淆矩](imgs_md/下样本的混淆矩阵.png)

```python
"""
混淆矩阵：x轴表示预测试，y轴表示观测值，可以通过混淆举证算出精度，召回率。
y轴
| (0, 0)表示TN，(0, 1)表示FP
|
| (1, 0)表示FN，(1, 1)表示TP
-----------------------------> x轴
"""
import itertools

lr = LogisticRegression(C=best_c, penalty='l1')
lr.fit(X_train_undersample, y_trian_undersample.values.ravel())
y_pred_undersample = lr.predict(X_test_undersample.values)

# 画出混淆矩阵
cnf_matrix = confusion_matrix(y_tes_undersample, y_pred_undersample)
np.set_printoptions(precision=2)
print('Recall metrix in the testing dataset:', cnf_matrix[1,1] / (cnf_matrix[1, 0] + cnf_matrix[1,1]))

def plot_confusion_matrix(cm, classes, title='Confusion Matrix', cmap=plt.cm.Blues):
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=0)
    plt.yticks(tick_marks, classes)

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j], horizontalalignment='center', color='white' if cm[i,j] > thresh else 'black')
    
    plt.tight_layout()
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')

# plot non-normalized confusionmatrix
class_names = [0, 1]
plt.figure()
plot_confusion_matrix(cnf_matrix, classes=class_names, title='Confusion Matrix')
plt.show()
```

**TNR过大**，下图是针对原始数据的样本进行预测，这里(0,1)表示FP，即没有问题的数据采用该模型预测出来了问题，这里的值不影响召回率，但是误杀了太多！！这个是**下采样的负作用**。

![于下采样的模型在原始数据集上的混淆矩](imgs_md/基于下采样的模型在原始数据集上的混淆矩阵.png)

```python
"""
基于下采样建立的模型在原始数据集上进行预测
"""
lr = LogisticRegression(C=best_c, penalty='l1')
lr.fit(X_train_undersample, y_trian_undersample.values.ravel())
y_pred = lr.predict(X_test.values)

# 画出混淆矩阵
cnf_matrix = confusion_matrix(y_test, y_pred)
np.set_printoptions(precision=2)
print('Recall metrix in the testing dataset:', cnf_matrix[1,1] / (cnf_matrix[1, 0] + cnf_matrix[1,1]))

# plot non-normalized confusionmatrix
class_names = [0, 1]
plt.figure()
plot_confusion_matrix(cnf_matrix, classes=class_names, title='Confusion Matrix')
plt.show()
```









***

[^1]: [机器学习知识体系之线性回归](https://www.cnblogs.com/wdsunny/p/6362582.html)
[^2]: [线性回归,逻辑回归与梯度下降]()https://www.cnblogs.com/futurehau/p/6105011.html

