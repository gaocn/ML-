##线性回归[^1]


$$
h_{\theta}(x) = \theta_0 +\theta_1 x_1 + ... + \theta_n x_n = \sum_{i=0}^{n}\theta_i x_i = \theta^T x，其中x_0=1 \\

预测值与真实值得关系：
y^{{i}} = \theta^T x^{(i)} + \epsilon^{(i)}
$$
假设误差$\epsilon^{(i)}$ 是独立同分布，通常认为服从均值为0方差为$\sigma^2$ 的高斯分布，则有
$$
\epsilon^{(i)} = y^{(i)} - \theta^T x^{(i)} \\
p(\epsilon^{(i)}) = \frac{1}{\sqrt{2 \pi} \sigma} exp(- \frac{(\epsilon^{(i)})^2}{2 \sigma^2}) \\

p(y^{(i)} | x^{(i)}； \theta) = \frac{1}{\sqrt{2 \pi} \sigma} exp(- \frac{(y^{(i)} - \theta^T x^{(i)} )^2}{2 \sigma^2})
$$
$p(y^{(i)} | x^{(i)}； \theta) $中$\theta$ 为要求出的参数值，表示需要找到$\theta$ 值与$x^{(i)}$ 的组合后得到的预测值接近$y^{(i)}$ 的概率，概率值越大$\theta$ 参数越好。如何求出$\theta$ ?利用最大似然函数求出，对于所有样本来说，当概率值最大值时的$\theta$ 求法为
$$
L(\theta)  = \prod_{i=1}^m p(y^{(i)} | x^{(i)}； \theta) =  \prod_{i=1}^m  \frac{1}{\sqrt{2 \pi} \sigma} exp(- \frac{(y^{(i)} - \theta^T x^{(i)} )^2}{2 \sigma^2}) \\
l(\theta) = logL(\theta) = \sum_{i=1}^{m} log[\frac{1}{\sqrt{2 \pi} \sigma} exp(- \frac{(y^{(i)} - \theta^T x^{(i)} )^2}{2 \sigma^2}) ] \\
= m \cdot log\frac{1}{\sqrt{2 \pi} \sigma} - \frac{1}{\sigma^2} \cdot \frac{1}{2}\sum_{i=1}^{m}(y^{(i)} - \theta^T x^{(i)})^2 \\
若要使l(\theta)最大，则上式后半部分的越小越好 \\
目标函数：J(\theta) = \frac{1}{2} \sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})^2
$$

$$
J(\theta) = \frac{1}{2} \sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})^2 = \frac{1}{2}(x\theta  - y)^T( x \theta - y) \\
\triangledown J(\theta) = \triangledown [\frac{1}{2}(\theta^T x^T - y^T)( x \theta - y)] 
= \triangledown [\theta^T x^T x \theta - \theta^T x^T y - y^T x \theta + y^Ty] 
\\=\frac{1}{2}[2x^Tx\theta - x^Ty - (y^Tx)^T] = x^Tx\theta - x^Ty = 0 （矩阵求导）\\
\theta = (x^Tx)^{-1} x^T y
$$

![线性回归举例](imgs_md/线性回归举例.png)

```python
import matplotlib.pyplot as plt
import numpy as np 
from sklearn import datasets

class LinearRegression(object):
    def __init__(self):
        self.theta = None
    
    def fit(self, X, y):
        print(X.shape)
        # Insert constant ones for bias weights
        # 第0列插入x.shape[0]行值为1
        X = np.insert(X, 0, 1, axis=1)
        print(X.shape)

        X_ = np.linalg.inv(X.T.dot(X))
        self.theta = X_.dot(X.T).dot(y)

    def predict(self, X):
        # Insert constant ones for bias weights
        X = np.insert(X, 0, 1, axis=1)
        y_pred = X.dot(self.theta)
        return y_pred
        
def mean_squared_error(y_true, y_pred):
    mse = np.mean(np.power(y_true - y_pred, 2))
    return mse

def main():
    # 加载数据集
    diabetes = datasets.load_diabetes()

    # 422 * 1，样本数422个，特征为1个 shape=(422, 1)
    X = diabetes.data[:, np.newaxis, 2]
    print(X.shape)

    # 划分训练集、测试集
    x_train, x_test = X[:-20], X[-20:]
    y_train, y_test = diabetes.target[:-20] , diabetes.target[-20:]

    clf = LinearRegression()
    clf.fit(x_train, y_train)
    y_pred = clf.predict(x_test)

    # 打印平均平方差误差(Mean Squred Error)
    print('Mean Squared Error: ', mean_squared_error(y_test, y_pred))
    plt.scatter(x_test[:, 0], y_test, color='black')
    plt.plot(x_test[:, 0], y_pred, color='blue', linewidth=3)
    plt.show()
```



##逻辑（Logistic）回归和梯度下降[^2]

逻辑回归实际上做了一个分类的事，通常逻辑回归适用于做二元分类。



S型函数
$$
g(z) = \frac{1}{1 + e^{-z}} \\
g'(x) = \frac{e^{-x}}{(1 + e^{-x})^2} = \frac{1}{1 + e^{-x}}  \cdot \frac{e^{-x}}{1 + e^{-x}} \\ =  \frac{1}{1 + e^{-x}}  \cdot (1-  \frac{1}{1 + e^{-x}}) = g(x)(1 - g(x))
$$
![sigmoid函数](imgs_md/sigmoid函数.png)

sigmoid函数能够将任何一个实数映射要[0,1]区间上，看做成一个概率值。

逻辑斯特回归实际上是将预测值$\theta^Tx$ 作为输入通过Sigmoid函数转换为概率值，实际上是将预测值用做分类任务。

$$
h_{\theta}(x) = g(\theta^T x) = \frac{1}{1 + e^{-\theta^Tx}}
$$

逻辑回归用于进行二分类
$$
P(y=1|x;\theta) = h_{\theta}(x) \\
P(y=0|x;\theta) = 1 - h_{\theta}(x) \\
p(y|x;\theta) = (h_{\theta}(x))^y(1- h_{\theta}(x))^{(1-y)} 求x,\theta的组合使得y值最接近 \\
L(\theta) = p(y|x:\theta) = \prod_{i=1}^{m}p(y^{(i)}|x^{(i)};\theta) =\prod_{i=1}^{m} (h_{\theta}(x^{(i)}))^{y^{(i)}}(1- h_{\theta}(x^{(i)}))^{(1-y^{(i)})} \\
l(\theta) = logL(\theta) = \sum_{i=1}^{m} y^{(i)}logh(x^{(i)}) + (1 - y^{(i)}) log(1- h(x^{(i)}))
$$
但是此时通过求导等于零食得到不到对应$\theta$ 的解的，因此可以通过==梯度下降==方法 









***

[^1]: [机器学习知识体系之线性回归](https://www.cnblogs.com/wdsunny/p/6362582.html)
[^2]: [线性回归,逻辑回归与梯度下降]()https://www.cnblogs.com/futurehau/p/6105011.html

