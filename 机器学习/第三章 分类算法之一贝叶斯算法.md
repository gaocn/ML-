#第三章 分类算法（分类器）

问题1：像R、Matlab、SAS等能能够处理大数据吗？能够用在生产环境吗？R、Matlab、SASS等主要是实验室工具，是用来建模的，主要用于研发模型；模型研发出来之后，实现方式可以使用C、Java等实现。例如：我们在实验室通过试管（R、Matlab）研制新的配方，在完成研制后，我们需要厂房进行集中大量生产，试想一下不可能在厂房使用大量试管生产新药。R、Matlab不适合处理大数据，当数据量到达百万级别时，处理起来已经很吃力了，因为R需要将数据加载到内存中，因此内存受限导致无法处理大量数据。虽然可以结合Hadoop使用R分析，但是这仅仅是一个数据接口；也有一些增强工具能够使R直接处理硬盘数据，例如：Oracle上的R软件可以在数据库中处理，但是大数据实际上会达到百亿级别，这种处理方式还是会很慢，而真正互联网上的大数据分析这是海量数据。

**处理大数据建模的方法**：1. 大数据变小数据抽样(聚类场景不适用)；2. 利用分布式集群结合并行算法；3. 降低问题精度，提高算法速度(例如求社交网络中完全子图，改为求满足一定度的子图)。

问题2：分类（判别分析）与聚类有什么差别？ 前者是有监督的有学习集，后者是无监督的无学习集。

**常见分类模型与算法**

| 分类算法        | 分类算法   |
| --------------- | ---------- |
| 线性判别法      | 距离判别法 |
| 贝叶斯分类器    | 决策树     |
| 支持向量机(SVM) | 神经网络   |

##1  线性判别法

**原理：**用一条直线来划分学习集，将学习集分别在直线的两侧，然后根据待测点在直线的哪一边决定它的分类。对于线性不可分的情况，



##2 距离判别法

**原理：**计算待测点与各类的距离（马氏距离），取最短者为其所属分类 



##3 K近邻（KNN）

如下图所示，根据K值不同得到不同的结果

- 如果k=3，绿色远点的最近的3个邻居是2个红色的小三角和1个蓝色的小正方形，少数服从多数，基于统计的方法，判定绿色的这个待分类点属于红色的三角形一类。
- 如果k=5，绿色圆点的最近的5个邻居是2个红色小三角和3个蓝色正方形，还是少数服从多数，基于统计的方法，哦安定绿色的这个待分类点属于蓝色的正方形一类。

![-近邻举](../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/imgs_md/K-%E8%BF%91%E9%82%BB%E4%B8%BE%E4%BE%8B.png)

**K-近邻算法描述**

1. 计算已知类别数据集中的点与当前未知类别属性数据集中的点的距离(欧式距离)；
2. 按照距离依次排序；
3. 选取与当前点距离最小的K个点；
4. 确定前K个点所在类别的出现频率；
5. 返回前K个点出现频率最高的类别作为当前点预测分类；

KNN算法本身简单有效，它是一种lazy-learning算法，分类器不需要使用训练集进行训练，训练时间复杂度为0。KNN分类的计算复杂度和训练集中的文档数目成正比，即如果训练集中文档总数为N，那么KNN的分类时间复杂度为O(N)。

K值得选择，距离度量和分类决策规则是该算法的三个基本要素。

**问题**：该算法在分类时有个主要的不足是，当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本上时，该样本的K个邻居中大容量样本占多数？解决办法是**不同的样本给予不同权重**。

```python
import numpy as np
class NearestNeighor:
    def __init__(self):
        pass
    def trai(self, X, y):
        """
        X is N*D where each row is an example. Y is 1-dimesion of size N
        """
        self.Xtr = X
        self.Ytr = y
    def predict(self, X):
        """
        X is N*D where each row is an example we wish to predict label for
        """
        num_test = X.shape[0]
        # make sure that the output type matches the input type
        Ypred = np.zeros(num_test, dtype=self.dtype)
        
        for i in range(num_test):
            # find the nearest trainging image to the i'th test image
            # usig the L1 distance(sum of absolute value different)
            distaces = np.sum(np.abs(self.Xtr - X[i, :]), axis=1)
            min_index = np.argmin(distaces)
            Ypred[i] = self.Ytr[min_index]
        return Ypred
```

##4 贝叶斯(Bayes)分类器

贝叶斯要解决的问题：逆向概率问题

正向概率：假设袋子里面有N个白球，M个黑球，你伸手进去摸一把，摸出黑球的概率？

逆向概率：如果我们事先不知道袋子里面黑白球的比例，而是闭着眼睛摸出一个（或几个）球，观察这些取出来的球的颜色之后，那么我们可以就此对袋子里面的黑白球的比例做出什么推测？

为什么使用贝叶斯？1. 现实世界本身就是不确定的，人类的观察能力是有局限性的；2. 我们日常生活所观察到的只是事情表面上的结果，因此我们需要提供一个猜测。

**范例：** 学校男生60%，女生40%，男生总是穿长裤子，而女生中一半穿长裤子一半穿裙子。

- 正向概率：随机选取一个学生，他穿长裤的概率和穿裙子的概率多大？
- 逆向概率：迎面走来一个穿长裤的学生，只看见他（她）穿的是长裤，而无法确定他的性别，你能够推断出他（她）是女生的概率有多大吗？

假设学校总人数为U

1. 穿长裤的男生人数为：U * P(Boy) * P(Pants|Boy)，其中P(Boy)=0.6，P(Pants|Boy)是条件概率表示所有男生中穿长裤的概率，这里是100%，因为所有男生总是穿长裤。
2. 穿长裤的女生人数为：U * P(Girl) * P(Pants|Girl)，其中P(Girl)=0.4，P(Pants|Girl)表示所有女生中床长裤的概率，这里为50%。

穿长裤的总人数为：U * P(Boy) * P(Pants|Boy) + U * P(Girl) * P(Pants|Girl)

问题：求穿长裤的人中是女生的概率？
$$
P(Girl|Pants) = \frac{U * P(Girl) * P(Pants|Girl) }{ 穿长裤的总数} \\
=\frac{U * P(Girl) * P(Pants|Girl) }{U * P(Boy) * P(Pants|Boy) + U * P(Girl) * P(Pants|Girl)} \\
=\frac{ P(Girl) * P(Pants|Girl) }{P(Boy) * P(Pants|Boy) + P(Girl) * P(Pants|Girl)}
$$


**贝叶斯公式**
$$
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
$$

P(A|B)称为**后验概率**表示给定观测样本数据B，假定A成立的概率；P(A)称为**先验概率**表示任意给定样本数据为A的概率。先验概率的计算可以根据大数定理计算，当抽样趋近于无穷时，频率趋近于先验概率。中P(B|A)(A)表示总体A出现的概率，而P(B|A)是在总体A中含有B特征的概率。

**范例：拼写纠正**

我们看到用户输入了一个不存在字典中的单词，我们需要去猜测“这个家伙到底真正想输入的单词是什么”，即我们要求的是：$P(我们猜测他想输入的单词|他实际输入的单词)$。假设用户实际输入的单词记为D即观测数据，我们猜测的有：P(h1|D)，P(h2|D)，...，P(hn|D)，统一为P(h|D)，而由贝叶斯公式可以得到

$$
P(h|D) = \frac{P(h)P(D|h)}{P(D)}
$$
其中P(h)表示单词h在字典中出现的频率，为特定猜测的先验概率。对于不同的具体猜测h1,h2,h3,...，P(D)都是一样的，所以比较P(h1|D)和P(h2|D)的时候，我们可以忽略这个常数。则有
$$
P(h|D) \propto P(h) * P(D|h)，\propto表示正比于
$$
对于给定观测数据，一个猜测是好是坏，取决于“这个猜测本身独立的可能性大小(先验概率，Prior)”和“这个猜测生成我们观测到的数据的可能性大小”。

例如用户输入tlp，到低是top还是tip?这个时候，当最大使然不能做出决定性判断时，先验概率就可以插手进来给出提示“既然你无法决定，那么我告诉你，一般来说top出现的程度高很多，所以更可能他想打的top”。

**拼写检查器实现** 
$$
arg \  maxc \  P(c|w) \rightarrow arg \ maxc \ \frac{P(w|c)P(c)}{P(w)}
$$

- P(c)表示文章中出现一个正确拼写单词c的概率，也就是说，在英文文章中，c出现的概率有多大；
- P(w|c)表示用户想键入c的情况下敲层w的概率，因为这个是代表用户会以多大的概率把c敲错成w；
- arg maxc表用用来枚举所有可能的c并且选取概率最大，即输入一个w，什么样的c词能够使得P(c|w)最大；

```python
import re, collections
# 把语料库中的单词全部抽取出来，装成小写，并且去除单词中间的特殊符号
def word(text):
    return re.findall('[a-z]+', text.lower())
# 计算词频，求出先验概率
def train(features):
    """
    要是遇到从未见过的新词怎么办？假如说一个词拼写完全正确，但是语料库中没有包含这个词，从而这个词永远不会出现在训练集中，于是，我们就要返回这个词的概率为0，这种情况不妙，因为概率为0代表这个事件不可能发生，而我们的概率模型中，我们期望用一个很小概率代表这种情况，即lambda:1，最小出现次数为1。
    """
    model = collections.defaultdict(lambda:1)
    for f in features:
        model[f] += 1
    return model
NWORDS = train(words(open('big.txt').read()))
alphabet = 'abcdefghijklmnopqrstuvwxyz'

# 编辑距离
# 两个词之间编辑距离定义为使用几次插入、删除、交换(相邻两个字母)、替换的操作从一个词变为另一个词。

# 返回所有单词w编辑距离为1的集合
def editsl(word):
    n = len(word)
    return set(
    	[word[0:i] + word[i+1:] for i in range(n)] + # deletion
        [word[0:i] + word[i+1] + word[i] + word[i+2] for i in range(n-1)] + # transpositon
        [word[0:i] + c + word[i+1:] for i in range(n) for c in alphabet]  + # alteration
        [word[0,i] + c + word[i: ] for i in range(n+1) for c in alphate] + # insertion
    ) 

# 返回所有与单词w编辑距离为2的集合
# 优化：在这些编辑距离小于2的集合中间，只把正确的词作为候选词，有许多事是无意义的单词
def edits2(word):
    return set(e2 for e1 in edits1(word) for e2 in alpha)

"""
正常来说把一个元音拼成另一个的概率要大于辅音（人人常把hello打成hella），把单词的第一个字母平措的概率会相对小等。为了简单起边，选择一个简单的方法：编辑距离为1的为正确单词比编辑距离为2的优先级高，而编辑距离为0的正确单词优先级比编辑距离为1的高
"""
def known(words):
    return set(w for w in words if w in NWORDS)

def known_edits2(words):
    return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in NWORDS)

#如果know(set)为空，candidate就会选取这个集合，而不继续计算后面
def correct(word):
    candidates = known([word]) or known(edits1(word)) or known_edits2(word) or [word]
    # 返回概率最大的单词
    return max(candidate, key=lambda w: NWORDS[w])
```

**模型比较理论** 

1. 最大似然：最符合观测数据的（即P(D|h)）最有优势；例如：扔一个硬币，观察到的是“正”，根据最大似然估计的方法，我们应该猜测这枚硬币扔出"正"的概率是1，因为这个才是能最大化P(D|h)的那个猜测
2. **奥卡姆剃刀**：P(h)较大的模型有较大的优势；例如：如果平面上有N个点，近似构成一条直线，但绝不精确地位于一条直线上，这时我们即可以用直线拟合，也可以用二阶多项式拟合，也可以用三阶多项式拟合，特别地，用N-1阶多项式便能够保证肯定能完美通过N个数据点（过拟合，把所有点都拟合了），那么这些可能的模型之中到低哪个是最靠谱的？根据奥卡姆剃刀原理，越是高阶的多项式越是不常见，$P(1) > P(2) >> P(n-1)$，因此常用的是1阶、2阶多项式。



**范例：垃圾邮件过滤** 

给定一封邮件，判定是否属于垃圾邮件。D表示这封邮件，注意D有N个单词组成，我们用h+来表示垃圾邮件，h-表示正常有奖，则有
$$
P(h+|D) = \frac{P(h+) * P(D | h+)}{P(D)} \\
P(h-|D) = \frac{P(h-) * P(D | h-)}{P(D)} \\
$$
其中P(h+)、P(h-)先验概率可以求出，例如在最近的1W封邮件中，有1000封是垃圾邮件，其余是正常邮件。其中P(D)在对比时是常数，可以去掉不考虑。P(D|h+)表示这封邮件是垃圾邮件并且是有单词D构成的。D里面包含有N个单词d1,d2,...，P(D|h+) = P(d1,d2,..,dn | h+)即在垃圾邮件当中出现跟我们目前这一封信一模一样的概率有多少？(概率极小)，因此对其进行扩展：
$$
P(d1,d2,..,dn | h+) = P(d1|h+) * P(d2|d1,h+) + P(d3|d2,d1,h+) + ...
$$

1. 假设di与di-1是完全条件无关的（==朴素贝叶斯假设特征之间是独立的，互不影响== ），则上式简化为
   $$
   P(d1,d2,..,dn | h+) = P(d1|h+) * P(d2|d1,h+) + P(d3|d2,d1,h+) + ... \\
   = P(d1|h+) * P(d2|h+) + P(d3|h+) + ...
   $$
   实际上单词间是有关系的，但是为了解决问题，我们提出变通的假设，这样就能得到解决方法，但是解决肯定是受影响的。

2. 对于$P(d1|h+) * P(d2|h+) + P(d3|h+) + ...$ ，只要统计di这个单词在垃圾邮件中出现的频率即可。

##5. 贝叶斯信念网络（Bayes Belief Network，BBN） 

朴素贝叶斯分类器需要特征之间互相独立的强条件，制约了模型的适用。

用有向无环图表达变量之间的依赖关系，变量用节点表示，依赖关系用边表示，祖先，父母和后代节点。 贝叶斯网络中的一个节点，如果它的父母节点已知，则它条件独立于它的所有非后代节点 

每个节点附带一个条件概率表（CPT），表示该节点和父母节点的联系概率 

从局部知识到全局知识的一个扩展，不同专家贡献不同的部分



**建模步骤**

1. 创建网络结构，有业务专家建立依赖关系网，要求是有向无环网；
2. 计算CPT，通过学习数据或者有专家根据经验给出；
3. 如果数据不完备，则需要进行训练计算（类似神经网络，采用梯度下降法） 

**CPT计算**

1. 如果节点X没有父母节点，则它的CPT乊包含先验概率P(X)；
2. 如果节点X只有一个父母节点Y，则CPT中包含条件概率P(X|Y)；
3. 如果节点X有多个父母节点Y1，Y2…，Yk，则CPT中包含条件概率P(X| Y1，Y2…，Yk)；

训练贝叶斯信念网络？与神经网络的训练类似

