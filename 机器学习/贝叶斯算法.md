##贝叶斯（Bayes）算法

贝叶斯要解决的问题：逆向概率问题

正向概率：假设袋子里面有N个白球，M个黑球，你伸手进去摸一把，摸出黑球的概率？

逆向概率：如果我们事先不知道袋子里面黑白球的比例，而是闭着眼睛摸出一个（或几个）球，观察这些取出来的球的颜色之后，那么我们可以就此对袋子里面的黑白球的比例做出什么推测？

为什么使用贝叶斯？1. 现实世界本身就是不确定的，人类的观察能力是有局限性的；2. 我们日常生活所观察到的只是事情表面上的结果，因此我们需要提供一个猜测。

**范例** 

学校男生60%，女生40%，男生总是穿长裤子，而女生中一半穿长裤子一半穿裙子。

- 正向概率：随机选取一个学生，他穿长裤的概率和穿裙子的概率多大？
- 逆向概率：迎面走来一个穿长裤的学生，只看见他（她）穿的是长裤，而无法确定他的性别，你能够推断出他（她）是女生的概率有多大吗？

假设学校总人数为U

1. 穿长裤的男生人数为：U * P(Boy) * P(Pants|Boy)，其中P(Boy)=0.6，P(Pants|Boy)是条件概率表示所有男生中穿长裤的概率，这里是100%，因为所有男生总是穿长裤。
2. 穿长裤的女生人数为：U * P(Girl) * P(Pants|Girl)，其中P(Girl)=0.4，P(Pants|Girl)表示所有女生中床长裤的概率，这里为50%。

穿长裤的总人数为：U * P(Boy) * P(Pants|Boy) + U * P(Girl) * P(Pants|Girl)

问题：求穿长裤的人中是女生的概率？
$$
P(Girl|Pants) = \frac{U * P(Girl) * P(Pants|Girl) }{ 穿长裤的总数} \\
=\frac{U * P(Girl) * P(Pants|Girl) }{U * P(Boy) * P(Pants|Boy) + U * P(Girl) * P(Pants|Girl)} \\
=\frac{ P(Girl) * P(Pants|Girl) }{P(Boy) * P(Pants|Boy) + P(Girl) * P(Pants|Girl)}
$$


**贝叶斯公式**
$$
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
$$


**范例：拼写纠正**

我们看到用户输入了一个不存在字典中的单词，我们需要去猜测“这个家伙到底真正想输入的单词是什么”，即我们要求的是：$P(我们猜测他想输入的单词|他实际输入的单词)$

假设用户实际输入的单词记为D即观测数据，我们猜测的有：P(h1|D)，P(h2|D)，...，P(hn|D)，统一为P(h|D)，而由贝叶斯公式可以得到
$$
P(h|D) = \frac{P(h)P(D|h)}{P(D)}
$$
其中P(h)表示单词h在字典中出现的频率，为特定猜测的先验概率。对于不同的具体猜测h1,h2,h3,...，P(D)都是一样的，所以比较P(h1|D)和P(h2|D)的时候，我们可以忽略这个常数。则有
$$
P(h|D) \propto P(h) * P(D|h)，\propto表示正比于
$$
对于给定观测数据，一个猜测是好是坏，取决于“这个猜测本身独立的可能性大小(先验概率，Prior)”和“这个猜测生成我们观测到的数据的可能性大小”。

例如用户输入tlp，到低是top还是tip?这个时候，当最大使然不能做出决定性判断时，先验概率就可以插手进来给出提示“既然你无法决定，那么我告诉你，一般来说top出现的程度高很多，所以更可能他想打的top”。

**拼写检查器实现** 
$$
arg \  maxc \  P(c|w) \rightarrow arg \ maxc \ \frac{P(w|c)P(c)}{P(w)}
$$

- P(c)表示文章中出现一个正确拼写单词c的概率，也就是说，在英文文章中，c出现的概率有多大；
- P(w|c)表示用户想键入c的情况下敲层w的概率，因为这个是代表用户会以多大的概率把c敲错成w；
- arg maxc表用用来枚举所有可能的c并且选取概率最大，即输入一个w，什么样的c词能够使得P(c|w)最大；

```python
import re, collections
# 把语料库中的单词全部抽取出来，装成小写，并且去除单词中间的特殊符号
def word(text):
    return re.findall('[a-z]+', text.lower())
# 计算词频，求出先验概率
def train(features):
    """
    要是遇到从未见过的新词怎么办？假如说一个词拼写完全正确，但是语料库中没有包含这个词，从而这个词永远不会出现在训练集中，于是，我们就要返回这个词的概率为0，这种情况不妙，因为概率为0代表这个事件不可能发生，而我们的概率模型中，我们期望用一个很小概率代表这种情况，即lambda:1，最小出现次数为1。
    """
    model = collections.defaultdict(lambda:1)
    for f in features:
        model[f] += 1
    return model
NWORDS = train(words(open('big.txt').read()))
alphabet = 'abcdefghijklmnopqrstuvwxyz'

# 编辑距离
# 两个词之间编辑距离定义为使用几次插入、删除、交换(相邻两个字母)、替换的操作从一个词变为另一个词。

# 返回所有单词w编辑距离为1的集合
def editsl(word):
    n = len(word)
    return set(
    	[word[0:i] + word[i+1:] for i in range(n)] + # deletion
        [word[0:i] + word[i+1] + word[i] + word[i+2] for i in range(n-1)] + # transpositon
        [word[0:i] + c + word[i+1:] for i in range(n) for c in alphabet]  + # alteration
        [word[0,i] + c + word[i: ] for i in range(n+1) for c in alphate] + # insertion
    ) 

# 返回所有与单词w编辑距离为2的集合
# 优化：在这些编辑距离小于2的集合中间，只把正确的词作为候选词，有许多事是无意义的单词
def edits2(word):
    return set(e2 for e1 in edits1(word) for e2 in alpha)

"""
正常来说把一个元音拼成另一个的概率要大于辅音（人人常把hello打成hella），把单词的第一个字母平措的概率会相对小等。为了简单起边，选择一个简单的方法：编辑距离为1的为正确单词比编辑距离为2的优先级高，而编辑距离为0的正确单词优先级比编辑距离为1的高
"""
def known(words):
    return set(w for w in words if w in NWORDS)

def known_edits2(words):
    return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in NWORDS)

#如果know(set)为空，candidate就会选取这个集合，而不继续计算后面
def correct(word):
    candidates = known([word]) or known(edits1(word)) or known_edits2(word) or [word]
    # 返回概率最大的单词
    return max(candidate, key=lambda w: NWORDS[w])
```



**模型比较理论** 

1. 最大似然：最符合观测数据的（即P(D|h)）最有优势；例如：扔一个硬币，观察到的是“正”，根据最大似然估计的方法，我们应该猜测这枚硬币扔出"正"的概率是1，因为这个才是能最大化P(D|h)的那个猜测
2. 奥卡姆剃刀：P(h)较大的模型有较大的优势；例如：如果平面上有N个点，近似构成一条直线，但绝不精确地位于一条直线上，这时我们即可以用直线拟合，也可以用二阶多项式拟合，也可以用三阶多项式拟合，特别地，用N-1阶多项式便能够保证肯定能完美通过N个数据点（过拟合，把所有点都拟合了），那么这些可能的模型之中到低哪个是最靠谱的？根据奥卡姆剃刀原理，越是高阶的多项式越是不常见，$P(1) > P(2) >> P(n-1)$，因此常用的是1阶、2阶多项式。



**范例：垃圾邮件过滤** 

给定一封邮件，判定是否属于垃圾邮件。D表示这封邮件，注意D有N个单词组成，我们用h+来表示垃圾邮件，h-表示正常有奖，则有
$$
P(h+|D) = \frac{P(h+) * P(D | h+)}{P(D)} \\
P(h-|D) = \frac{P(h-) * P(D | h-)}{P(D)} \\
$$
其中P(h+)、P(h-)先验概率可以求出，例如在最近的1W封邮件中，有1000封是垃圾邮件，其余是正常邮件。其中P(D)在对比时是常数，可以去掉不考虑。P(D|h+)表示这封邮件是垃圾邮件并且是有单词D构成的。D里面包含有N个单词d1,d2,...，P(D|h+) = P(d1,d2,..,dn | h+)即在垃圾邮件当中出现跟我们目前这一封信一模一样的概率有多少？(概率极小)，因此对其进行扩展：
$$
P(d1,d2,..,dn | h+) = P(d1|h+) * P(d2|d1,h+) + P(d3|d2,d1,h+) + ...
$$

1. 假设di与di-1是完全条件无关的（==朴素贝叶斯假设特征之间是独立的，互不影响== ），则上式简化为
   $$
   P(d1,d2,..,dn | h+) = P(d1|h+) * P(d2|d1,h+) + P(d3|d2,d1,h+) + ... \\
   = P(d1|h+) * P(d2|h+) + P(d3|h+) + ...
   $$
   实际上单词间是有关系的，但是为了解决问题，我们提出变通的假设，这样就能得到解决方法，但是解决肯定是受影响的。

2. 对于$P(d1|h+) * P(d2|h+) + P(d3|h+) + ...$ ，只要统计di这个单词在垃圾邮件中出现的频率即可。













