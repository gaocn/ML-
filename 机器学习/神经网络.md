##神经网络

什么是人工智能？感知+理解==>具有决策的能力。

> 学习的能力，才是职能的本质

大数据时代造就了人工智能的发展，使得机器有了学习的能力，进而推动的人工智能的发展。深度学习在90年代就已经出现了，那个时代计算能力太慢，为被人所熟知。2012年一位牛人使用卷积神经网络在[IMAGENET](www.image-net.org)上比赛分类图片得到效果要比机器学习高十几个百分点，至此以后卷积神经网络一炮打响。首先需要了解神经网络，然后再转向深度学习。

为什么人工智能的技术这么厉害？

将以前的机器学习算法进行改进，深度学习和机器学习有什么关系？机器学习领域是非常庞大的，在庞大的机器学习算法中有一类算法叫做神经网络，学者将神经网络进行了扩展和改进，使得神经网络变得扩大一些，加入更多的细节，利用这个扩大的神经网络能够让我们需要预测、分类的效果更好。下图是百度深度学习研究院的ppt，当数据规模小的时候，深度学习和传统人工智能算法差别不大，当数据规模增加时深度学习算法的效果就比传统人工智能算法要好。

![度学习与机器学](imgs_md/深度学习与机器学习.png)

###图像分类：计算机视觉核心任务

深度学习和计算机视觉关系密切，一张图像在计算机中被表示为三维组的形式，每个像素的值从0到255（值越大，表示该像素点越亮）。例如：300\*100\*3，其中300、100可以分别看成是高、宽，图像是有颜色通道的，彩色图有三种颜色通道RGB，因这个RGB通道就是第三个维度，即每一点像素是由三个通道的颜色叠加而形成的。

相同物体在不同角度、不同亮度、物体形态的改变、部分遮蔽、汇入等的情况下如何识别并检测出来

![](imgs_md/遮蔽.png)

![像识别的挑](imgs_md/图像识别的挑战.png)

深度学习的步骤，与机器学习是一样的

1. 收集大量数据并给定标签；
2. 训练一个分类器；
3. 测试，评估

###K-近邻分类算法

如下图所示，根据K值不同得到不同的结果

- 如果K=3，绿色远点的最近的3个邻居是2个红色的小三角和1个蓝色的小正方形，少数服从多数，基于统计的方法，判定绿色的这个待分类点属于红色的三角形一类。
- 如果k=5，绿色圆点的最近的5个邻居是2个红色小三角和3个蓝色正方形，还是少数服从多数，基于统计的方法，哦安定绿色的这个待分类点属于蓝色的正方形一类。

![-近邻举](imgs_md/K-近邻举例.png)

**K-近邻算法描述**

1. 计算已知类别数据集中的点与当前未知类别属性数据集中的点的距离；
2. 按照距离依次排序；
3. 选取与当前点距离最小的K个点；
4. 确定前K个点所在类别的出现频率；
5. 返回前K个点出现频率最高的类别作为当前点预测分类；

KNN算法本身简单有效，它是一种lazy-learning算法，分类器不需要使用训练集进行训练，训练时间复杂度为0。KNN分类的计算复杂度和训练集中的文档数目成正比，即如果训练集中文档总数为N，那么KNN的分类时间复杂度为O(N)。

K值得选择，距离度量和分类决策规则是该算法的三个基本要素。

问题：该算法在分类时有个主要的不足是，当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本上时，该样本的K个邻居中大容量样本占多数？解决办法是不同的样本给予不同权重。

数据集：CIFAR-10

**对于图像数据如何计算距离**
$$
L1 \ Distance：d_1(I_1, I_2) = \sum_p|I_1^p - I_2^p|
$$
这种方式计算距离时，我们也把背景考虑进来的，因此在使用K-近邻时，由于背景噪声的影响会导致结果不准确，因此使用K-近邻计算图片相似度的结果不会很好，是不能使用的，因此常用的方法是神经网络。

![1 Distanc](imgs_md/L1 Distance.png)

```python
import numpy as np
class NearestNeighor:
    def __init__(self):
        pass
    def trai(self, X, y):
        """
        X is N*D where each row is an example. Y is 1-dimesion of size N
        """
        self.Xtr = X
        self.Ytr = y
    def predict(self, X):
        """
        X is N*D where each row is an example we wish to predict label for
        """
        num_test = X.shape[0]
        # make sure that the output type matches the input type
        Ypred = np.zeros(num_test, dtype=self.dtype)
        
        for i in range(num_test):
            # find the nearest trainging image to the i'th test image
            # usig the L1 distance(sum of absolute value different)
            distaces = np.sum(np.abs(self.Xtr - X[i, :]), axis=1)
            min_index = np.argmin(distaces)
            Ypred[i] = self.Ytr[min_index]
        return Ypred
```

**超参数**

这里的距离参数在训练模型时是可以改变的，通过选择不同的距离公式得到不同的距离，称这些可变参数为超参数。如下使用两种方式计算距离
$$
L1（Manhattan）distance：d_1(I_1, I_2) = \sum_p|I_1^p - I_2^p| \\
L2（Euclidean）distance：d_2(I_1, I_2) = \sqrt{\sum_p(I_1^p - I_2^p)^2}
$$
问题：

1. 对于距离如何设定？
2. 对于K近邻的K该如何选择？
3. 如果有的话，其他的超参数该怎么设定呢？

错误的想法是将划分的训练集和测试集多次使用找到最佳参数，事实上测试集非常宝贵，只能在最终验证参数时使用。正确的方式是通过*交叉验证* 找到最优参数。如下图所示，首先将原始数据拆分为训练集和测试集，然后对训练集进行拆分将其中的一份作为验证集。例如：将训练集拆分成5份，其中fold5作为验证集用来调节模型的参数，其他4份作为训练集，则通过多次训练，第一次：<fold1,fold2,fold3,fold4>作为训练集，<fold5>作为验证集，第二次：<fold1,fold3,fold5,fold4>作为训练集，<fold2>作为验证集，第三次：<fold2,fold5,fold1,fold4>作为训练集，<fold3>作为验证集，...如此循环进行多次，用来找到最佳参数，交叉采用不同数据训练的原因是数据有可能不纯，存在有一块或几块数据可能有问题可能存在误差值、异常值导致训练结果偏高或偏低，而采用不同方式交叉训练，将不同训练得到的参数求平均可以消除偏高或偏低的情况，这样得到的模型就比较科学、平均化不至于产生极端效果。

![叉验](imgs_md/交叉验证.png)

###线性分类

如下图所示，这对给定的一张$32*32*3$的图片输入，要做一个分类任务，我们希望找到一组最优权重参数$W$能够输出针对输入在每个类别的得分值。

![性分类](imgs_md/线性分类1.png)

假设类别为10种，则$f(x,W)$的值应该为$10*1$的列向量，而对于给定的输入项将其拉伸为$3072*1$一维列向量，则如下图所示，则权重参数就为$10*3072$的矩阵，若存在截距项b，则b为$10*1$的列向量。由此构成得分函数$f(x,W)=Wx + b$ 

![性分类](imgs_md/线性分类2.png)

例如：将输入为$2*2$的图像拉伸为$4*1$的列向量$x$，得到的权重举证为$W$则得到每一个类别的得到。从图中可以看出在权重矩阵W中的值有大小、有正负，值越大说明对应像素点影响程度比较大，值越小则相应像素点的影响程度较小；而对于权重值为负，则说明对应的像素点的影响是消极的，而正值则表示对应像素点有积极作用。在得到一组得分值后，我们从中去除得分最大的那一类获胜，也就是最终的预测结果。

![性分类3(得分函数举例](imgs_md/线性分类3(得分函数举例).png)

采用权重数据与输入数据进行组合，就相当于完成一个线性分类的工作，每一个类别就能画一条线出来，如下图所示，通过直线划分不同类别，就相当于找出决策边界，通过权重参数找出倾斜度，然后通过b找出截距值，最后就能够得出如下的划分区域。

![性分类](imgs_md/线性分类4.png)

###损失函数

上述得分函数得到的结果有可能是不对的，即预测结果是错的，因此模型需要做到知错能改，将错误给改正过来的能力。如下图所示，对于三个不同的输入图片，根据得分函数得到的结果如下。对于当前输入的得分结果，计算其属于正确类型得分值与其他所有错误类别得分值得差异情况，若错误分类得分比正确分类得分大，这个时候就是分类错误，这个就是损失值，加1意思是定义可容忍程度，例如当前决策模型下，最小容忍损失是1，则预测错误的误差要小于1才能没有损失。
$$
cat = max(0，5.1 - 3.2 + 1) + max(0，-1.7-3.2 + 1) = 2.9 +0 = 2.9 \quad \qquad \qquad\\
car = max(0， 1.3 - 4.9 + 1) + max(0，2.0 - 4.9 + 1) = 0 + 0 = 0 (\lt0认为没有损失) \\
gua = max(0， 2.2 - (-3.1) + 1) + max(0， 2.5 - (-3.1) +1) = 5.3 +5.6 = 10.9 \qquad
$$
损失函数能够判断模型的好坏，损失值越大，模型预测效果越差。

![失函数举](imgs_md/损失函数举例.png)

SVM损失函数
$$
L_i = \sum_{j \ne y_i} max(0,s_j - s_{y_i} + \delta)，其中\delta为可容忍程度
$$
score越大，则距离错误分类的值就越远，此时损失值就越小。

![失函数](imgs_md/损失函数1.png)

**损失函数**
$$
f(x,W) = Wx \\
L = \frac{1}{N}\sum_{i=1}^{N}\sum_{j \ne y_i} max(0，f(x_i；W)_j - f(x_i；W)_{y_i})
$$




















