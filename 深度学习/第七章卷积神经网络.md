## 第七章 卷积神经网络

BP神经网络处理图形识别的问题：

1. 全连接造成需要确定的权值太多，需要很多样本去训练，计算困难。一个解决方法就是**减少权值**，常用方法有局部连接、权值共享(若干个隐层神经元与对应像素点的权值都是一样的)。下图中原点为隐层神经元。

![](/Users/gaowenwen/ML-/深度学习/md_img/减少权值训练.jpg)

上述方法的理论基础来自于：1962年Hubel和Wiesel通过对猫视觉皮层绅胞的研究，提出了**感受野(receptive field)** 的概念，1984年日本学者Fukushima基于**感受野**概念提出的神经认知机 (neocognitron)可以看作是卷积神经网络的第一个实现网络，也是感受野概念在人工 神绉网络领域的首次应用。神经认知机将一个视觉模式分解成许多子模式(特征)， 然后进入分层递阶式相连的特征平面进行处理，它试图将视觉系统模型化，使其能够在即使物体有位移或轻微变形的时候，也能完成识别。即在动物身上神经元也不是全接连的，权值共享的生物学解释。

2.  BP层数太多时会有梯度消失问题，BP中采用反向传播算法时，反向误差每反向传一层都会乘以激活函数的倒数，由于BP中的激活函数是Sigmoid而Sigmoid的倒数是小于1的，因此会导致反向传播的学习信号越来越弱。因此卷积神金网络需要尽量减少Sigmod函数的使用。此外使用**权值共享**可以使学习信号不要变弱，因为越往下权值不会越分散，权值会越来越少，使得学习的信号越来越集中，因此权值共享也可以解决梯度消失问题。
3.  若采用局部连接造成的问题是中间过度的地方不会太平滑，造成**边缘过度不平滑**问题。解决方法是**采样窗口彼此重叠**。

![](/Users/gaowenwen/ML-/深度学习/md_img/边缘过度不平滑.jpg)



### **卷积神经网络的网络结构**

​	卷积神经网络是一个多层的神经网络，每层由多个二维平面组成，而每个平面由多个独立神经元组成。

​	卷积神经网络的概念示范：输入图像通过和三个可训练的滤波器和可加偏置进行卷积，滤波过程如图一，卷积后在C1层产生三个特征映射图，然后特征映射图中每组的四个像素再进行求和，加权值，加偏置，通过一个Sigmoid函数得到三个S2层的特征映射图。这些映射图再进过滤波得到C3层。这个层级结构再和S2一样产生S4。最终，这些像素值被光栅化，并连接成一个向量输入到传统的神经网络，得到输出。

​       一般地，C层为特征提取层，每个神经元的输入与前一层的局部感受野相连，并提取该局部的特征，一旦该局部特征被提取后，它与其他特征间的位置关系也随之确定下来；S层是特征映射层，网络的每个计算层由多个特征映射组成，每个特征映射为一个平面，平面上所有神经元的权值相等。特征映射结构采用影响函数核小的sigmoid函数作为卷积网络的激活函数，使得特征映射具有位移不变性。

​       此外，由于一个映射面上的神经元共享权值，因而减少了网络自由参数的个数，降低了网络参数选择的复杂度。卷积神经网络中的每一个特征提取层（C-层）都紧跟着一个用来求局部平均与二次提取的计算层（S-层），这种特有的两次特征提取结构使网络在识别时对输入样本有较高的畸变容忍能力。

![](/Users/gaowenwen/ML-/深度学习/md_img/卷积神经网络的网络结构.jpg)



### **卷积神经网络案例--LeNET5**

[LeNet-5, convolutional neural networks](http://yann.lecun.com/exdb/lenet/index.html)，手写数字识别[^1]奠定了卷积神经网络的基础。下图中卷积神经网络除了输入层外，有7层。

**原始输入**为$32 \times 32$的手写数字图片且要求数字在图形中间$20 \times 20$的位置，像素的颜色用灰度表示，灰度范围为[-0.1, 1.175]以确保图片的所有像素的灰度值的平均值为0，有利于训练效果。

C1卷积层有6个卷积平面、卷积特征图、卷积和，每个卷积和上的神经元对应于输入图片中的一个$5 \times 5$的窗口，采样窗口平移的步长为1(垂直和水平方向)，则可以得到卷积平面维度为$28 \times 28$ 且每个神经元的**权值共享**(较少计算量，减少样本数)，其中单个神经元对应的一套权值又称为**滤波器**，因此同一个卷积平面上的神经元使用同一套滤波器/权值。因此6个卷积平面使用6套不同的滤波器或权值。假设第1~6个卷积平面使用的滤波器对应为t1~t6，不同的滤波器代表其抽出的特征是不一样的。则C1中总共有$156 = (5*5+1) * 6$个参数(每个卷积平面共享权值)，有$122,304 = (5 * 5 + 1)*28*28*6$个连接，其中加1位偏置值的连接。

S2为子采样层(Subsampling，用于压缩数据)有6个特征平面，每个特征平面维度为$14 \times 14$，子采样层的每个神经元通过**把卷积平面上每4个相邻像素(不采用重叠窗口)加在一起乘以一个权值再加上一个偏置值**得到，且同一个子采样层神经元乘以的权值和加上的偏置值都一样。S2中总共有$12 = 2 * 6$个参数，有$5,880 = (4 + 1) *14 * 14 * 6 $个连接。子采样层又称为**池化层(Pool)**。

C3卷积层层有16个特征平面，每个特征平面维度为$10 \times 10$，如下图所示，假设16个平面的编号为0~15，对于第0号平面中的一个神经元由S2层中的0~2号特征面的3个$5 \times 5$的像素(75个像素)，采用窗口重叠和共享权值的方式得到C3的第0号特征平面，依次类推得到全部16个特征平面。

![](/Users/gaowenwen/ML-/深度学习/md_img/卷积神经网络LeNet5的C3层映射规则.png)

上图中可以看出前0~5个平面采用S2的三个平面构造，6~14个平面采用S2的四个平面构造，第15个平面则使用S2的全部平面构造，为什么没有采用对称方式得到得到所有平面而采用这种卷积的方式？LeCun的解释是为了打破对称性[^1]，因为特征往往不是以对称的方式呈现的，有些是高层特征有的是底层特征。为了能够把各种层面的特征都能抽取出来，因此采用一种打破平衡的方式采取特征，C3层的目的就是打破平衡使得不同层面的特征可以被抽取出来， 卷积和比较多可以使得抽取出来的特征更加丰富。C3层总共有需要训练的参数为：
$$
1,516 = [(5*5)*3+1]*6 + [(5*5)*4+1]*9 + [(5*5)*6+1]*1
$$
C3卷积层总共的连接数为：
$$
151,600=[(5*5)*3+1] * 10 * 10 * 6 + [(5*5)*4+1]*10*10*9 + [(5*5)*6+1]*10*10*1
$$
![](/Users/gaowenwen/ML-/深度学习/md_img/LeNET5.png)

S4子采样层总共有16个特征平面，每个特征平面的维度为$5\times5$，特征平面中的每个神经元C3层对应平面中的$2\times 2$个像素加在一起乘以一个权值再加上一个偏置值得到，且同一个子采样层神经元乘以的权值和加上的偏置值都一样。

C5卷积层有120张平面，每个神经元与S4层的全部16个平面的的$5\times5$像素。由于S4层特征图的大小也为$5\times5$（同滤波器一样），故C5特征平面的维度为$1\times1$：这构成了S4和C5之间的全连接。之所以仍将C5标示为卷积层而非全相联层，是因为如果LeNet-5的输入变大，而其他的保持不变，那么此时特征图的维数就会比1*1大。C5层有$48,120=[(5 * 5) * 16 + 1] * 120$个可训练连接，由于是全连接可训练的参数与连接数一样。

F6层为普通的BP网络的隐层，其中有84个神经元与S6中的神经元全连接，在BP网络中没有局部窗口和权值共享，总共连接数为$10,164 = (120 + 1)*84$。因此可以看出，越往后去权值参数越多，是倒三角型，与BP神经网络相反，在误差反向传播过程中，学习信号往下传播的时候不会被过于分散出现学习梯度消失的情况。

OUTPUT输出层有10个输出神经元，分别对应于数字0~9各自的结果，输出结果越大为相应的数字的可能性越大。输出层由欧式径向基函数（Euclidean Radial Basis Function）单元组成，每类一个单元，每个有84个输入。换句话说，每个输出RBF单元计算输入向量和参数向量之间的欧式距离。输入离参数向量越远，RBF输出的越大。一个RBF输出可以被理解为衡量输入模式和与RBF相关联类的一个模型的匹配程度的惩罚项。用概率术语来说，RBF输出可以被理解为F6层配置空间的高斯分布的负log-likelihood。给定一个输入模式，损失函数应能使得F6的配置与RBF参数向量（即模式的期望分类）足够接近。这些单元的参数是人工选取并保持固定的（至少初始时候如此）。这些参数向量的成分被设为-1或1。虽然这些参数可以以-1和1等概率的方式任选，或者构成一个纠错码，但是被设计成一个相应字符类的7*12大小（即84）的格式化图片。这种表示对识别单独的数字不是很有用，但是对识别可打印ASCII集中的字符串很有用。

​      使用这种分布编码而非更常用的“1 of N”编码用于产生输出的另一个原因是，当类别比较大的时候，非分布编码的效果比较差。原因是大多数时间非分布编码的输出必须为0。这使得用sigmoid单元很难实现。另一个原因是分类器不仅用于识别字母，也用于拒绝非字母。使用分布编码的RBF更适合该目标。因为与sigmoid不同，他们在输入空间的较好限制的区域内兴奋，而非典型模式更容易落到外边。

​        RBF参数向量起着F6层目标向量的角色。需要指出这些向量的成分是+1或-1，这正好在F6 sigmoid的范围内，因此可以防止sigmoid函数饱和。实际上，+1和-1是sigmoid函数的最大弯曲的点处。这使得F6单元运行在最大非线性范围内。必须避免sigmoid函数的饱和，因为这将会导致损失函数较慢的收敛和病态问题。

**PS：C1、C3、C5卷积层没有激活函数或激活函数为pureline，而S2、S4层的激活函数为Sigmoid，F6的激活函数为双曲正切函数。**

### CNN的训练[^3][^7]

训练算法与传统的BP算法差不多，主要包括4步，这4步被分为两个阶段：

**第一阶段，向前传播阶段**

1.  从样本集中取出一个样本$(X,Y_p)$，将X输入网络；

2. 计算相应的实际输出$O_p$，在此阶段信息从输入层经过逐级的变换，传送到输出层。这个过程也是网络在完成训练后正常运行时执行的过程。在此过程中，网络执行的计算(实际就是输入与每层的权值矩阵点乘得到最后的输出结果)：
   $$
   O_p = F_n(...(F_2(F_1(X_pW_1)W_2)...)W_n)
   $$


**第二阶段，向后传播阶段**

1. 算实际输出$O_p$与相应的理想输出的$Y_p$的差；
2. 按极小化误差的方法反向传播调整权矩阵；

LeNet5的实现可以参见[^6],有关LeNet5的简化系统可以[参考文章](https://blog.csdn.net/celerychen2009/article/details/8973218)。



卷积神经网络的特点是当图像平移时不会影响最后的结果，这就是ALphaGo使用卷积神经网络的原因，下围棋时不考虑四个边的情况而只关心中间的情况，当中间棋局平移的时候最优下法也会平移，这就**平移不变性**。例如：图形中的数字无论怎么平移任然会识别为相应的数字。



### 深度卷积神经网络—ImageNet[^2]

​	ImageNet 是一个计算机视觉系统识别项目， 是目前丐界上图像识别最大的数据库，是美国斯坦福的计算机科学家，模拟人类的识别系统建立的，能够从图片识别物体。 ImageNet LSVRC图像识别大赛素有国际“计算机视觉奥林匹克”之称。数据集包含 大约1000多万张各种图片，被分为1000个分类，参赛者训练分类器，在测试数据上取 得最高辨识正确率者为优胜。ImageNet 缔造者李飞飞，美国斯坦福大学计算机科学系副教授，现任美国斯坦福大学人工智能实验室和视觉实验室主任，主要研究斱向为机器学习、计算机视觉、认知计算神经学。李飞飞认为：无人车自劢驾驶是计算机学习人脑的极佳研究，基本目标是让计算机学习人脑做决定的方法。 

​	Hinton率领的谷歌团队多次夺冠，2015年微软团队获得多项第一。微软研究团队包 括何恺明、张祥雨、任少卿和孙剑四位成员，他们所设计的系统名为“深度残差学习图像识别”，据说该系统会在即将发表的报告中详细介终。该技术主要因其复杂性著称。神经网络的深度超过了150层。这个框架能减轻极深度网络的优化和收敛。当网络深度在原有基础上大幅度加深时，“深度残差网”的准确性就会显现出来。 

![](/Users/gaowenwen/ML-/深度学习/md_img/屏幕快照 2018-07-01 下午12.21.35.png)

除去输入层，下图神经网络总共有8层，与LeNet5的结构图不同的是，在LeNet5中卷积层和采样层是分开的，而下图中卷积层和采样层混合在一起作为一层，由于卷积层和采样层都是线性的，因此线性叠加后的结果也是线性的，直接计算后，最后应用采样层的激活函数求得结果就行。下图中，只能看到卷积层是看不到子采样层的。

Pooling池化层：平均化，将若干个大小不同的小池子中的水放到一个大的池子中，原来小池子中的可能高低不同，放在同一个池子中后就比较平均。

输入层为$224\times 224 \times 3$，其中3为3元颜色表示彩色图片，三种不同颜色RGB，可以分解出不同的亮度，每一种亮度的数值不同，可以认为有一张红色、一张绿色、一张蓝色的图片叠加构成，称为厚度。

C1卷积成中的一个神经元的采样窗口为$11 \times 11 \times 3$表示在三张图像上各自采样$11 \times 11$的像素，采样窗口平移的步长为4(在LeNet5中移动步长为1，由于LeNet5数据量小而这里若采用1则训练时间太长)，得到C1卷积层为$55 \times 55$的平面，总共有$(48+48)=96$个卷积平面(厚度)，这里分为上面有48个平面，下面有48个平面，论文指出训练方法是采用两路GPU计算，而采用96个平面应该与每个GPU的核心有关，若一个GPU的核心为48则一张平面刚好可以扔给一个GPU去做，加快训练速度。C2卷积层中的一个神经元通过对48张卷积平面中的$5\times5$的像素进行采样得到，得到$128+128=256$个卷积平面，每个卷积平面为$27\times27$ 。C3卷积层的一个神经元对256个卷积平面中的$3\times3$的像素进行，得到$13\times13$的卷积平面，厚度为192+192=384。C4，C5卷积层类似。

C5卷积成与C6层采用类似BP网络的全连接层，得到$2048*2=4096$个神经元，C6层和C7层也采用全连接得到4096个神经元。C7层与输出层同样采用全连接方式得到1000个神经元，即1000个分类预测结果来判断输入是属于哪一个分类。



### GPU(Graphics Processor Unit)

GPU用于向量运算，而CPU流行的是4核、8核等，而GPU核心很多例如英伟达的由960核心，因此GPU加工处理图像速度就会非常快，因此GPU可以用来做高性能计算。

- CUDA（Compute Unified Device Architecture），是显卡厂商[NVIDIA](https://baike.baidu.com/item/NVIDIA)推出的运算平台。 CUDA™是一种由NVIDIA推出的通用[并行计算](https://baike.baidu.com/item/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97/113443)架构，该架构使[GPU](https://baike.baidu.com/item/GPU)能够解决复杂的计算问题。 它包含了CUDA[指令集架构](https://baike.baidu.com/item/%E6%8C%87%E4%BB%A4%E9%9B%86%E6%9E%B6%E6%9E%84)（[ISA](https://baike.baidu.com/item/ISA)）以及GPU内部的并行计算引擎。 开发人员现在可以使用[C语言](https://baike.baidu.com/item/C%E8%AF%AD%E8%A8%80)来为CUDA™架构编写程序，C语言是应用最广泛的一种高级编程语言。所编写出的程序可以在支持CUDA™的处理器上以超高性能运行。CUDA3.0已经开始支持[C++](https://baike.baidu.com/item/C%2B%2B)和[FORTRAN](https://baike.baidu.com/item/FORTRAN)。
- Tesla GPU的20系列产品家族基于代号为“Fermi”的下一代[CUDA架构](https://baike.baidu.com/item/CUDA%E6%9E%B6%E6%9E%84)，支持技术与企业计算所“必备”的诸多特性，其中包括C++支持、可实现极高精度与可扩展性的ECC存储器以及7倍于Tesla 10系列GPU的双精度性能。Tesla? C2050与C2070 GPU旨在重新定义高性能计算并实现超级计算的平民化。与最新的四核CPU相比，Tesla C2050与C2070计算处理器以十分之一的成本和二十分之一的功耗即可实现同等超级计算性能。
- Xeon Phi是由美国英特尔公司于2012年11月推出的首款60核处理器，Xeon Phi并非传统意义上的英特尔处理器(CPU)，它更像是不CPU协同工作的GPU (图形处理器，显卡)，其基于英特尔消费级GPU技术Larrabee，不过该项目已经于 2009年被取消。但英特尔仍然需要Larrabee技术，从而在超级计算机市场与Nvidia竞 争，因为更简单、更与业的GPU处理器可以更有效地处理某些超级计算任务，从而提高性能幵减少能耗。 



### CNN的数学背景[^4][^5]

为什么CNN运用在图像识别和语音识别上效果很好呢？CNN的特性：

    1. 抗扭曲(diffeomorphis，微分同胚);
    2. 平移能力强(Translation)；

为什么会称为卷积神经网络？卷积为一种数学变换，卷积层的操作与数学上的卷积变换是一样的，因此称为卷积。卷积操作：$ \sum f(x)g(x-u)$

傅里叶变化类似泰勒变换(将任何函数分解为幂级数)，可以把图像或声音分解为不同的频率，可以将任何函数分解为傅里叶级数，傅里叶级数中每一项是一个三角函数，三角函数的系数不同。
$$
 F(X) = \sum_i { A_i * cos(x)  + B_i * sin(x) }
$$
傅里叶变化其实是把不同的信号分解成不同频率的成分，通常我们只会对某一个范围的频率感兴趣，例如：
高频部分、低频部分或某一段频率范围的成分。傅里叶变换代表特征提取。

傅里叶变换与卷积操作有着密切的关联：$F(f*g) = F(f) * F(g)$
两者不同的是：傅里叶变换是全空间变换，而卷积变换是局部操作，卷积操作与小波变换相似。傅里叶变换、小波变换和卷积变换非常适合于处理信号分解相关问题，因此可以用于对图像分解、信号分解。实际上卷积神经网络是基于经验建立起来的。



### 参考文献

[^1]: Gradient based learning applied to document recognition
[^2]: ImageNet Classification with Deep Convolutional Neural Networks
[^3]: 如何训练CNN：Notes on Convolutional Neural Networks
[^4]: CNN的数学背景：Understanding Convolutional Neural Networks (2016), J. Koushik ，CNN的由来，为什么适用于图像识别和语音识别？
[^5]: CNN的数学背景：Understanding Deep Convolutional Networks，CNN的由来，为什么适用于图像识别和语音识别？
[^6]: [LeNet5实现](https://www.codeproject.com/Articles/16650/Neural-Network-for-Recognition-%20of-Handwritten-Digi)
[^7]: [卷积神经网络算法的一个实现](https://www.cnblogs.com/fengfenggirl/p/cnn_implement.html)