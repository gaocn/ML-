## 第七章 卷积神经网络

BP神经网络处理图形识别的问题：

1. 全连接造成需要确定的权值太多，需要很多样本去训练，计算困难。一个解决方法就是**减少权值**，常用方法有局部连接、权值共享(若干个隐层神经元与对应像素点的权值都是一样的)。下图中原点为隐层神经元。

![](/Users/gaowenwen/ML-/深度学习/md_img/减少权值训练.jpg)

上述方法的理论基础来自于：1962年Hubel和Wiesel通过对猫视觉皮层绅胞的研究，提出了**感受野(receptive field)** 的概念，1984年日本学者Fukushima基于**感受野**概念提出的神经认知机 (neocognitron)可以看作是卷积神经网络的第一个实现网络，也是感受野概念在人工 神绉网络领域的首次应用。神经认知机将一个视觉模式分解成许多子模式(特征)， 然后进入分层递阶式相连的特征平面进行处理，它试图将视觉系统模型化，使其能够在即使物体有位移或轻微变形的时候，也能完成识别。即在动物身上神经元也不是全接连的，权值共享的生物学解释。

2.  BP层数太多时会有梯度消失问题，BP中采用反向传播算法时，反向误差每反向传一层都会乘以激活函数的倒数，由于BP中的激活函数是Sigmoid而Sigmoid的倒数是小于1的，因此会导致反向传播的学习信号越来越弱。因此卷积神金网络需要尽量减少Sigmod函数的使用。此外使用**权值共享**可以使学习信号不要变弱，因为越往下权值不会越分散，权值会越来越少，使得学习的信号越来越集中，因此权值共享也可以解决梯度消失问题。
3.  若采用局部连接造成的问题是中间过度的地方不会太平滑，造成**边缘过度不平滑**问题。解决方法是**采样窗口彼此重叠**。

![](/Users/gaowenwen/ML-/深度学习/md_img/边缘过度不平滑.jpg)



### **卷积神经网络的网络结构**

​	卷积神经网络是一个多层的神经网络，每层由多个二维平面组成，而每个平面由多个独立神经元组成。

​	卷积神经网络的概念示范：输入图像通过和三个可训练的滤波器和可加偏置进行卷积，滤波过程如图一，卷积后在C1层产生三个特征映射图，然后特征映射图中每组的四个像素再进行求和，加权值，加偏置，通过一个Sigmoid函数得到三个S2层的特征映射图。这些映射图再进过滤波得到C3层。这个层级结构再和S2一样产生S4。最终，这些像素值被光栅化，并连接成一个向量输入到传统的神经网络，得到输出。

​       一般地，C层为特征提取层，每个神经元的输入与前一层的局部感受野相连，并提取该局部的特征，一旦该局部特征被提取后，它与其他特征间的位置关系也随之确定下来；S层是特征映射层，网络的每个计算层由多个特征映射组成，每个特征映射为一个平面，平面上所有神经元的权值相等。特征映射结构采用影响函数核小的sigmoid函数作为卷积网络的激活函数，使得特征映射具有位移不变性。

​       此外，由于一个映射面上的神经元共享权值，因而减少了网络自由参数的个数，降低了网络参数选择的复杂度。卷积神经网络中的每一个特征提取层（C-层）都紧跟着一个用来求局部平均与二次提取的计算层（S-层），这种特有的两次特征提取结构使网络在识别时对输入样本有较高的畸变容忍能力。

![](/Users/gaowenwen/ML-/深度学习/md_img/卷积神经网络的网络结构.jpg)



### **卷积神经网络案例--LeNET5**

[LeNet-5, convolutional neural networks](http://yann.lecun.com/exdb/lenet/index.html)，手写数字识别[^1]奠定了卷积神经网络的基础。下图中卷积神经网络除了输入层外，有7层。

**原始输入**为$32 \times 32$的手写数字图片且要求数字在图形中间$20 \times 20$的位置，像素的颜色用灰度表示，灰度范围为[-0.1, 1.175]以确保图片的所有像素的灰度值的平均值为0，有利于训练效果。

C1卷积层有6个卷积平面、卷积特征图、卷积和，每个卷积和上的神经元对应于输入图片中的一个$5 \times 5$的窗口，则可以得到卷积平面维度为$28 \times 28$ 且每个神经元的**权值共享**(较少计算量，减少样本数)，其中单个神经元对应的一套权值又称为**滤波器**，因此同一个卷积平面上的神经元使用同一套滤波器/权值。因此6个卷积平面使用6套不同的滤波器或权值。假设第1~6个卷积平面使用的滤波器对应为t1~t6，不同的滤波器代表其抽出的特征是不一样的。则C1中总共有$156 = (5*5+1) * 6$个参数(每个卷积平面共享权值)，有$122,304 = (5 * 5 + 1)*28*28*6$个连接，其中加1位偏置值的连接。

S2为子采样层(Subsampling，用于压缩数据)有6个特征平面，每个特征平面维度为$14 \times 14$，子采样层的每个神经元通过**把卷积平面上每4个相邻像素(不采用重叠窗口)加在一起乘以一个权值再加上一个偏置值**得到，且同一个子采样层神经元乘以的权值和加上的偏置值都一样。S2中总共有$12 = 2 * 6$个参数，有$5,880 = (4 + 1) *14 * 14 * 6 $个连接。子采样层又称为**池化层(Pool)**。

C3卷积层层有16个特征平面，每个特征平面维度为$10 \times 10$，如下图所示，假设16个平面的编号为0~15，对于第0号平面中的一个神经元由S2层中的0~2号特征面的3个$5 \times 5$的像素(75个像素)，采用窗口重叠和共享权值的方式得到C3的第0号特征平面，依次类推得到全部16个特征平面。

![](/Users/gaowenwen/ML-/深度学习/md_img/卷积神经网络LeNet5的C3层映射规则.png)

上图中可以看出前0~5个平面采用S2的三个平面构造，6~14个平面采用S2的四个平面构造，第15个平面则使用S2的全部平面构造，为什么没有采用对称方式得到得到所有平面而采用这种卷积的方式？LeCun的解释是为了打破对称性[^1]，因为特征往往不是以对称的方式呈现的，有些是高层特征有的是底层特征。为了能够把各种层面的特征都能抽取出来，因此采用一种打破平衡的方式采取特征，C3层的目的就是打破平衡使得不同层面的特征可以被抽取出来， 卷积和比较多可以使得抽取出来的特征更加丰富。C3层总共有需要训练的参数为：
$$
1,516 = [(5*5)*3+1]*6 + [(5*5)*4+1]*9 + [(5*5)*6+1]*1
$$
C3卷积层总共的连接数为：
$$
151,600=[(5*5)*3+1] * 10 * 10 * 6 + [(5*5)*4+1]*10*10*9 + [(5*5)*6+1]*10*10*1
$$
![](/Users/gaowenwen/ML-/深度学习/md_img/LeNET5.png)

S4子采样层总共有16个特征平面，每个特征平面的维度为$5\times5$，特征平面中的每个神经元C3层对应平面中的$2\times 2$个像素加在一起乘以一个权值再加上一个偏置值得到，且同一个子采样层神经元乘以的权值和加上的偏置值都一样。

C5卷积层有120张平面，每个神经元与S4层的全部16个平面的的$5\times5$像素。由于S4层特征图的大小也为$5\times5$（同滤波器一样），故C5特征平面的维度为$1\times1$：这构成了S4和C5之间的全连接。之所以仍将C5标示为卷积层而非全相联层，是因为如果LeNet-5的输入变大，而其他的保持不变，那么此时特征图的维数就会比1*1大。C5层有$48,120=[(5 * 5) * 16 + 1] * 120$个可训练连接，由于是全连接可训练的参数与连接数一样。

F6层为普通的BP网络的隐层，其中有84个神经元与S6中的神经元全连接，在BP网络中没有局部窗口和权值共享，总共连接数为$10,164 = (120 + 1)*84$。因此可以看出，越往后去权值参数越多，是倒三角型，与BP神经网络相反，在误差反向传播过程中，学习信号往下传播的时候不会被过于分散出现学习梯度消失的情况。

OUTPUT输出层有10个输出神经元，分别对应于数字0~9各自的结果，输出结果越大为相应的数字的可能性越大。输出层由欧式径向基函数（Euclidean Radial Basis Function）单元组成，每类一个单元，每个有84个输入。换句话说，每个输出RBF单元计算输入向量和参数向量之间的欧式距离。输入离参数向量越远，RBF输出的越大。一个RBF输出可以被理解为衡量输入模式和与RBF相关联类的一个模型的匹配程度的惩罚项。用概率术语来说，RBF输出可以被理解为F6层配置空间的高斯分布的负log-likelihood。给定一个输入模式，损失函数应能使得F6的配置与RBF参数向量（即模式的期望分类）足够接近。这些单元的参数是人工选取并保持固定的（至少初始时候如此）。这些参数向量的成分被设为-1或1。虽然这些参数可以以-1和1等概率的方式任选，或者构成一个纠错码，但是被设计成一个相应字符类的7*12大小（即84）的格式化图片。这种表示对识别单独的数字不是很有用，但是对识别可打印ASCII集中的字符串很有用。

​      使用这种分布编码而非更常用的“1 of N”编码用于产生输出的另一个原因是，当类别比较大的时候，非分布编码的效果比较差。原因是大多数时间非分布编码的输出必须为0。这使得用sigmoid单元很难实现。另一个原因是分类器不仅用于识别字母，也用于拒绝非字母。使用分布编码的RBF更适合该目标。因为与sigmoid不同，他们在输入空间的较好限制的区域内兴奋，而非典型模式更容易落到外边。

​        RBF参数向量起着F6层目标向量的角色。需要指出这些向量的成分是+1或-1，这正好在F6 sigmoid的范围内，因此可以防止sigmoid函数饱和。实际上，+1和-1是sigmoid函数的最大弯曲的点处。这使得F6单元运行在最大非线性范围内。必须避免sigmoid函数的饱和，因为这将会导致损失函数较慢的收敛和病态问题。

**PS：C1、C3、C5卷积层没有激活函数或激活函数为pureline，而S2、S4层的激活函数为Sigmoid，F6的激活函数为双曲正切函数。**

### LeNet5的训练





### LeNet5的实现









### 参考文献

[^1]: Gradient based learning applied to document recognition
[^2]: ImageNet Classification with Deep Convolutional Neural Networks
[^3]: Notes on Convolutional Neural Networks
[^4]: Understanding Convolutional Neural Networks (2016), J. Koushik 
[^5]: Understanding Deep Convolutional Networks