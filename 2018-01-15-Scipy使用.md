#Scipy使用（一）

```python
import numpy as np
from scipy import linalg, optimize

#查看方法的注释
np.info(optimize.fmin)
np.source(optimize.fmin)

```

##一、Scipy文件操作（[`scipy.io`](https://docs.scipy.org/doc/scipy/reference/tutorial/io.html)）

用于将数据导入导出成不同格式的文件。常用的是与MATLAB数据文件进行交互。

| 方法                                     | 说明                        |
| -------------------------------------- | ------------------------- |
| loadmat(file_name[,mdict, appendmat])  | 加载MATLAB数据文件              |
| savemat(file_name, mdict[, appednmat]) | 将字典或数据保存为MATLAB数据文件.mat格式 |
| whosmat(file_name[, appendant])        | 查看MATLAB文件的数据信息           |

```python
from scipy import io as sio
from numpy as np

mat_contents = sio.loadmat('octave_a.mat')
#{'a': array([[[  1.,   4.,   7.,  10.],
#        [  2.,   5.,   8.,  11.],
#        [  3.,   6.,   9.,  12.]]]),
# '__version__': '1.0',
# '__header__': 'MATLAB 5.0 MAT-file, Created on: 2013-02-17 21:02:11 UTC',
# '__globals__': []}

vect = np.arange(10)
sio.savemat('np_vect.mat', {'vect': vect})

#whosmat返回元组列表，每一数组对应一个元组(name, shape, data_type)
sio.whosmat('octave_a.mat')
#[('a', (1, 3, 4), 'double')]

#读取图片文件与matplotlib中的imread类似
#若报错，需要安装pillow包
from scipy import misc 
imdata = misc.imread("demo.jpg")
```

##二、线性代数（[`scipy.linalg`](https://docs.scipy.org/doc/scipy/reference/tutorial/linalg.html)）

###2.1 matrix VS ndarray？ 

在numpy中有两种用于矩阵计算的类，一种是numpy.matrix类，另一种是numpy.ndarray类。numpy.matrix类提供了矩阵运算常用的接口，例如: `*` 在matrix对象中就是矩阵的乘法而在ndarray对象中这不是，ndarray中需要使用a_ndarr.dot(b_ndarr)。`尽管matrix使用很方便，但是不建议使用，基于ndarray对象提供的接口更多，而引入matrix类会产生混淆！ 矩阵的所有运算一样能够使用ndarray计算出来`   

```python
import numpy as np
from scipy import linalg

#matrix对象
mat = np.mat('[1 2;3 4]')
mat.T
mat.I
mat * mat.T

#ndarray对象
A = np.array([[1,2],[3,4]])
linalg.inv(A)
b = A.T
A.dot(b)

```

###2.2 linalg基本函数

**方阵的逆：inv**

设矩阵A的逆矩阵为B，则满足AB=I且A为方阵，其中I为单位矩阵，通常$B=A^{-1}$ 

```python
import numpy as np
from scipy import linalg
A = np.array([[1,3,5],[2,5,1],[2,3,8]])
#array([[1, 3, 5],
#      [2, 5, 1],
#      [2, 3, 8]])
linalg.inv(A)
#array([[-1.48,  0.36,  0.88],
#      [ 0.56,  0.08, -0.36],
#      [ 0.16, -0.12,  0.04]])
# double check
A.dot(linalg.inv(A)) 
#array([[  1.00000000e+00,  -1.11022302e-16,  -5.55111512e-17],
#      [  3.05311332e-16,   1.00000000e+00,   1.87350135e-16],
#      [  2.22044605e-16,  -1.11022302e-16,   1.00000000e+00]])
```

**求解线性方程组：solve(A, b)**

对于线性方程组Ax = b，若方程有解即$|A| \ne 0$ 则有$x = A^{-1}b$

```python
import numpy as np
from scipy import linalg
A = np.array([[1, 2], [3, 4]])
#array([[1, 2],
#      [3, 4]])
b = np.array([[5], [6]])
#array([[5],
#      [6]])
linalg.inv(A).dot(b)  # slow
#array([[-4. ],
#      [ 4.5]])
#A.dot(linalg.inv(A).dot(b)) - b  # check
#array([[  8.88178420e-16],
#      [  2.66453526e-15]])

#方法2：速度更快
np.linalg.solve(A, b) 
array([[-4. ],
      [ 4.5]])
#check
A.dot(np.linalg.solve(A, b)) - b 
#array([[ 0.],
#      [ 0.]])
```

**行列式：det(A)**

[行列式](https://baike.baidu.com/item/%E8%A1%8C%E5%88%97%E5%BC%8F)在数学中，是一个函数，其定义域为det的矩阵A，取值为一个标量，写作det(A)或 | A | 

```python
import numpy as np
from scipy import linalg
A = np.array([[1,2],[3,4]])
linalg.det(A)
#-2.0
```

**范数：norm(a, ord=None, axis=None, keepdims=False)**

计算向量或矩阵范数，根据ord的不同norm函数能够返回不同的矩阵范数和向量范数。

- a为(M, )或者(M, N)类型的ndarray，若axis为None则a必须是1-D或2-D的数组；
- ord为范数的阶数，取值为{非零整数，inf，-inf，‘fro’}；
- axis取值为{int，2-tuple of ints，None}，若为int则表示沿哪个轴计算向量范数，若为元组则对应2-D矩阵的矩阵范数；若为None则a是1-D计算向量范数，a是2-D计算矩阵范数。
- keepdims当设置为True时， the axes which are normed over are left in the result as dimensions with size one

对于向量和矩阵对应的范数计算公式如下
$$
||X|| =
\begin{cases}
    max|X_i| \quad ord = inf \\
    min|X_i| \quad ord=-inf \\
    (\sum_{i}|X_i|^{ord})^{\frac{1}{ord}} \quad |ord| \lt \propto
\end{cases} \\


||A|| = 
\begin{cases}
    max_i\sum_j|a_{ij}| \quad ord = inf \\
    min_i\sum_j|a_{ij}| \quad ord = -inf \\
    max_j\sum_i|a_{ij}| \quad ord = 1 \\
    min_j\sum_i|a_{ij}| \quad ord = -1 \\
    max \sigma_i| \quad ord = 2 \\
    min \sigma_i| \quad ord = -2 \\
    \sqrt{trace(A^HA)} \quad ord = 'fro' \\
\end{cases}
$$

```python
import numpy as np
from scipy import linalg
A=np.array([[1,2],[3,4]])
#array([[1, 2],
#      [3, 4]])

linalg.norm(A)
#5.4772255750516612
linalg.norm(A,'fro') # frobenius norm is the default
#5.4772255750516612
linalg.norm(A,1) # L1 norm (max column sum)
#6
linalg.norm(A,-1)
#4
linalg.norm(A,np.inf) # L inf norm (max row sum)
#7    
```

**线性方程的最小二乘(least-squares)解与伪逆(Pseudo-Inverse)**

假设数据$y_i$ 与数据$x_i$ 是相关的，通过一组系数(Coefficients)$c_i$ 和函数$f_i(x_i)$ 满足模型
$$
y_i = \sum_{j}c_{j}f_{j}(x_i) + \varepsilon_i，其中\varepsilon_i 是数据中的不确定项
$$
采用最小二乘法就是找到一组系数$c_i$ 使得真实值$y_i$ 与预测值之差的平方（消除符号影响）和最小
$$
J(c) = \sum_i|y_i - \sum_{j}c_{j}f_{j}(x_i) |^2
$$
求最小值只需要分别对系数$c_i$ 求偏导数，并令偏导数等于零，求得一组解就能够使得J(c)的值最小
$$
\frac{\partial J}{\partial c_{n}^{*}} = 0 = \sum_i(y_i - \sum_{j}c_{j}f_{j}(x_i))(-f_{n}^{*}(x_i)) \\ or \\
\sum_{j}c_{j}\sum_{i}f_{j}(x_i)f_{n}^{*}(x_{i}) = \sum_{i}y_{i}f_{n}^{*}(x_i) \\
A^HAc = A^Hy \quad where \quad \{A_{ij}\} = f_{j}(x_i)  \quad and \quad  A^HA是可逆的\\
\downarrow \\
c = (A^HA)^{-1}A^Hy = A^{+}y \quad where \; 若A不可逆，则 A^{+}是A的伪逆
$$
因为$A_{ij}=f_j(x_i)$， 则定义的模型可以写为：$y = Ac + \varepsilon$ . `linalg.lstsq(A, y)`方法能够在给定A和y的情况下计算出系数c。此外l`inalg.pinv`和`linalg.pinv2`会计算出矩阵A的伪逆$A^+$

范例：下面例子使用`linalg.lstsq(A, y)` 和`inalg.pinv` 解决数据拟合问题(data-fitting)，下面数据是基于模型，其中给$y_i$ 添加了噪音
$$
y_i = c_1e^{-x_i} + c_2x_i \quad where \ x_{i} = 0.1i \ ，i \in [1, 10]  \ c_1 = 5，c_2 = 4  
$$

```python
import numpy as np
from scipy import linalg
import matplotlib.pyplot as plt

c1, c2 = 5.0, 2.0
i = np.r_[1:11]
#array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])
xi = 0.1*i
#array([ 0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ])
yi = c1*np.exp(-xi) + c2*xi
#array([ 4.72418709,  4.49365377,  4.3040911 ,  4.15160023,  4.0326533 ,
#        3.94405818,  3.88292652,  3.84664482,  3.8328483 ,  3.83939721])
#带噪声的y值
zi = yi + 0.05 * np.max(yi) * np.random.randn(len(yi))

A = np.c_[np.exp(-xi)[:, np.newaxis], xi[:, np.newaxis]]
#array([[ 0.90483742,  0.1       ],
#       [ 0.81873075,  0.2       ],
#       [ 0.74081822,  0.3       ],
#       [ 0.67032005,  0.4       ],
#       [ 0.60653066,  0.5       ],
#       [ 0.54881164,  0.6       ],
#       [ 0.4965853 ,  0.7       ],
#       [ 0.44932896,  0.8       ],
#       [ 0.40656966,  0.9       ],
#       [ 0.36787944,  1.        ]])

# lstsq返回值
# c:        最小二乘的解，为(N,)或(N, K)ndarray
# residues：残差和，squared 2-norm for each column in ``y-Ac``
# rank:     矩阵A的秩
# sigma:    矩阵A的奇异值
c, resid, rank, sigma = linalg.lstsq(A, zi)
#(array([ 5.00278791,  2.10108782]),
# 0.47110363682170298,
# 2,
# array([ 2.58763467,  1.02933937]))

#用模型预测的值
xi2 = np.r_[0.1:1.0:100j]
yi2 = c[0]*np.exp(-xi2) + c[1]*xi2

#查看拟合效果
plt.plot(xi,zi,'x',xi2,yi2)
plt.axis([0,1.1,3.0,5.5])
plt.xlabel('$x_i$')
plt.title('Data fitting with linalg.lstsq')
plt.show()
```

**伪逆矩阵（广义逆矩阵）** 

| 伪逆矩阵方法       | 说明                              |
| ------------ | ------------------------------- |
| linalg.pinv  | Moore-Penrose利用最小二乘法lstsq求解伪逆矩阵 |
| linalg.pinv2 | 利用SVD分解求伪逆矩阵                    |
| linalg.pinvh | 求Hermitian矩阵的伪逆矩阵               |

奇异矩阵或非方阵的矩阵不存在逆矩阵，但可以用函数`linalg.pinv` or `linalg.pinv2` 求矩阵的伪逆矩阵。令A为M*N的矩阵，称矩阵G是A的广义逆矩阵（伪逆矩阵）。
$$
A为M \times N的矩阵，\qquad \qquad \qquad\qquad \qquad \qquad\qquad \qquad \qquad\\
1. 若M > N，则A的广义逆矩阵为：A^+ = (A^HA)^{-1}A^H  \\
2. 若M < N，则A的广义逆矩阵为：A^\# =A^H (AA^H)^{-1}   \\
3. 若M = N，则A的广义逆矩阵为：A^\# =A^+ = A^{-1}  \quad
$$

###2.3 特征值（Eigenvalues）、特征向量（Eigenvectors）

对于矩阵A存在常数$\lambda$以及对应向量v，满足等式$Av = \lambda v$ ，对于$N \times N$的矩阵存在N个特征值（不一定不同），满足多项式的$|A - \lambda I| = 0$根。特征向量v也称为右特征向量，区别于左特征向量$v_{L}^{H}$ ，左特征向量满足
$$
\ v{L}^{H} A = \lambda v{L}^{H} \quad or \quad  A^H v{L} = \lambda^{*} v{H} ，其中H表示共轭向量|矩阵（Hermitian conjugation），\lambda^{*}是\lambda的共轭向量
$$
此外，`linalg.eig` 方法能够解决广义的特征值问题，对于方阵A，B，满足
$$
A v = \lambda B v  \qquad  \qquad \ \ \ 右特征向量\\
A^H v_{L} = \lambda^{*}B^{H}v_{L}   \qquad 左特征向量
$$
同样可以求出对应的特征值和特征向量。实际上，标准的特征值和特征向量是在B=I的情况下求出的。当求解出特征向量后，我们就能够得到矩阵A的一个分解
$$
A = B V  \Lambda V_{-1} \qquad V是列特征向量构成矩阵，\Lambda是有特征值构成的对角矩阵
$$
根据定义，特征向量只取决于常量因子$\lambda$ ，在Scipy中特征向量的的常量因子满足
$$
||v||^2 = \sum_{i}v_{i}^2 = 1
$$
`范例` ：
$$
A = \left [
\begin{matrix}
    1 & 5 & 2 \\
    2 & 4 & 5 \\
    3 & 6 & 2
\end{matrix}
\right ] \\
|A-\lambda I| = (1 - \lambda)[(4 - \lambda)(2 - \lambda)-6] - 5[2(2-\lambda) - 2]  \\ 
+ 2[12 - 3(4 - \lambda)]  \qquad \qquad \\
= - \lambda^3 + 7\lambda^2 + 8\lambda - 3 \qquad \qquad \qquad \  \\
求得： \lambda_{1} = 7.9579，\lambda_{2} = -1.2577，\lambda_{3}=0.2997  \qquad  \quad
$$
**linalg.eig(a, b=None, left=False, right=True, overwrite_a=False, overwrite_b=False, check_finite=True)**

**linalg.eigvals(a, b=None, overwrite_a=False, check_finite=True) 仅仅返回特征值w**

- a要计算特征值和特征向量的矩阵
- b为在计算广义特征值和特征向量时，满足Av = xBv中的矩阵B
- left为True表示计算左特征向量，right默认为True表示返回右特征向量
- overwrite_a是否覆盖a矩阵，overwrite_b是否覆盖b矩阵，为False会提高计算速度；
- check_finite为True是会检查矩阵中的数是不是finite number，设置为False会提高计算速度

​     `返回值` 

- w, 类型为特征值构成(M, )的ndarray
- vl，规范化的左特征向量(M, M)的ndarray，当left=True是返回；
- vr，规范化的右特征向量(M, M)的ndarray，当right=True是返回；

```python
import numpy as np
from scipy import linalg
A = np.array([[1, 2], [3, 4]])
la, v = linalg.eig(A)
l1, l2 = la
# eigenvalues
#(-0.372281323269+0j) (5.37228132327+0j)

v
#array([[-0.82456484, -0.41597356],
#       [ 0.56576746, -0.90937671]])
# first eigenvector
print(v[:, 0])   
[-0.82456484  0.56576746]
 # second eigenvector
print(v[:, 1])  
[-0.41597356 -0.90937671]

# eigenvectors are unitary
print(np.sum(abs(v**2), axis=0))  
[ 1.  1.]

v1 = np.array(v[:, 0]).T
#check the computation
print(linalg.norm(A.dot(v1) - l1*v1)) 
3.23682852457e-16
```

###2.4 矩阵分解(Decomposition)

**奇异值分解（SVD，Singular Value Decomposition）** 

奇异值分解可以看成是特征值问题的扩展，`针对非方阵` ,  设A为$M \times N$ 的矩阵，满足$A^HA、AA^H分别是N \times N、M \times M 的 Hermitian方阵$ ，Hermitian方阵的特征值是`实数且是非负的`，并且在Hermitian方阵$A^HA、AA^H$中`最多有min(M, N)个相同的非零特征值` 。设这些非负特征值为$\sigma_{i}^2$ ，那么这些特征值开根号后的即为矩阵A的奇异值，矩阵$A^HA$ 的列特征向量构成了$N \times N$的unitary矩阵$V $ ，而矩阵$AA^H$ 的列特征向量构成的$M \times M$ 的unitary矩阵U，奇异值构成的$M \times N$对角矩阵$\Sigma$  ，则：
$$
A = U \Sigma V^H \\
 hermitian \  matrix \ D \ 满足 D^H = D \\
 unitary \ matrix \ D \ 满足 D^HD = I = DD^H \rightarrow D^{-1} = D^H
$$
就是A的一个奇异分解。每一个矩阵都存在一个奇异值分解。有时候奇异值称为矩阵的A的谱(spectrum)。`linalg.svd` 返回$U，\sigma_{i} 数组，V^H $ ，`linalg.diagsvd` 返回奇异值构成的对角矩阵$\Sigma$ `linalg.svdvals` 仅仅返回矩阵的奇异值。

```python
import numpy as np
from scipy import linalg
A = np.array([[1,2,3],[4,5,6]])
#array([[1, 2, 3],
#      [4, 5, 6]])

M,N = A.shape
U,s,Vh = linalg.svd(A)
Sig = linalg.diagsvd(s,M,N)

U, Vh = U, Vh
#array([[-0.3863177 , -0.92236578],
#      [-0.92236578,  0.3863177 ]])
#Sig
#array([[ 9.508032  ,  0.        ,  0.        ],
#      [ 0.        ,  0.77286964,  0.        ]])
#Vh
#array([[-0.42866713, -0.56630692, -0.7039467 ],
#      [ 0.80596391,  0.11238241, -0.58119908],
#      [ 0.40824829, -0.81649658,  0.40824829]])

U.dot(Sig.dot(Vh)) #check computation
#array([[ 1.,  2.,  3.],
#      [ 4.,  5.,  6.]])
```

**LU分解**

对于矩阵$M \times N $ 的A，LU分解后得到
$$
A = P \ L \ U  \\
其中P是M \times M的permutation \ matrix（单位矩阵按行随机排列得到的矩阵） \\
L是M \times K的下三角矩阵[K = min(M, N)]  \\
U是K \times N的上三角矩阵
$$
LU分解通常用于解决simultaneous equations，并且等式左边不变而右边经常变动
$$
Ax_i = b_i，（对于不同的b_i） \\
\downarrow \\
PLUx_i = b_i
$$

>An initial time spent factoring A allows for very rapid solution of similar systems of equations in the future. If the intent for performing LU decomposition is for solving linear systems then the command `linalg.lu_factor` should be used followed by repeated applications of the command `linalg.lu_solve` to solve the system for each new right-hand-side.

**linalg.lu(a, permute_l=False, overwrite_a=False, check_finite=True)**

- 默认返回值为P， L，U
- 若permute_l为True，则返回pl，U

**linalg.lu_factor(a, overwrite_a=False, check_finite=True)**

- 计算矩阵A 的pivoted LU分解
- 返回值为N*N的矩阵LU和N长度的数组piv

**linalg.lu_solve(lu_and_piv, b, trans=0, overwrite_b=False, check_finite=True)**

- lu_and_piv为元组(LU， piv)
- b为Ax=b方程组中矩阵b
- trans取值为{0, 1, 2}分别对应：a x   = b， a^T x = b，a^H x = b
- 返回值为x数组即方程组的解

**Cholesky decomposition**

Cholesky分解是LU分解的一个特例，针对Hermitian矩阵（$A=A^H \ and \ x^H A x \ge 0 \ for \ all \ x$），则A的分解可以写为
$$
A = U^H U  \qquad (U是上三角矩阵)\\
A = L L^H  \qquad (L是下三角矩阵，L = U^H)
$$
同样存在方法`linalg.cholesky` ，` linalg.cho_factor`，`linalg.cho_solve`

**QR分解**

QR分解又称为a polar decomposition，对于任意$M \times N$ 的矩阵能够找到$M \times N$ 的unitary矩阵Q和$M \times N$ 上三角矩阵R，满足
$$
A = QR \\
若A的SVD分解是已知的，则有 A = U \Sigma V^H = QR，则有Q = U  \ ， R = \Sigma V^h
$$

| QR分解相关方法                     | 说明                                       |
| ---------------------------- | ---------------------------------------- |
| linalg.qr(a)                 | 返回Q, R, P                                |
| linalg.qr_multiply(a, c)     | 进行QR分解，并将Q乘以c返回，返回Qc, R, P               |
| linalg.qr_update(Q, R, u, v) | If ``A = Q R`` is the QR factorization of ``A``, return the QR factorization of ``A + u v**T`` for real ``A`` or ``A + u v**H`` for complex ``A`` ，返回Q1, R1 |
| linalg.qr_delete             | If ``A = Q R`` is the QR factorization of ``A``, return the QR  factorization of ``A`` where ``p`` rows or columns have been removed starting at row or column ``k``. |
| linalg.qr_insert             | If ``A = Q R`` is the QR factorization of ``A``, return the QR factorization of ``A`` where rows or columns have been inserted starting at row or column ``k``. |

##三、最优方法（[`scipy.optimize`](https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html) ）



###3.1 梯度下降法（Gradient Descent）

当目标函数是凸函数时，梯度下降法的解是全局解。一般情况下，其解不保证是全局最优解，梯度下降法的速度也未必是最快的。梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是”最速下降法“。梯度下降法在接近最优解的区域收敛速度明显变慢，利用梯度下降法求解需要很多次的迭代。

牛顿法的缺点：1. 靠近极小值时收敛速度减慢，如下图所示；2 . 直线搜索时可能会产生一些问题；3. 可能会“之字形”地下降。

例子：对于一个线性回归（Linear Logistics）模型，假设h(x)是要拟合的函数，$J(\theta)$ 为损失函数，$\theta$ 是参数，要迭代求解的值，$\theta$ 求解出来了，最终要拟合的函数$h(\theta)$ 就求出来的。
$$
h(\theta) = \sum_{j=0}^{n}\theta_jx_j ，其中n为特征个数\\
J(\theta) = \frac{1}{2m} \sum_{i=1}^{m}(y_i - h_{\theta}(x^i))^2，其中m为训练集样本个数 \\
梯度向量 \ grad(J(\theta_1, \theta_2, ..., \theta_n)) = \bigtriangledown J(\theta_1, \theta_2, ..., \theta_n) = (\frac{\partial{J(\theta)}}{\partial{\theta_1}}, \frac{\partial{J(\theta)}}{\partial{\theta_2}}, ... \frac{\partial{J(\theta)}}{\partial{\theta_n}})\\ 
沿梯度方向函数变化最快的方向！
$$
**(1). 批量梯度下降法(Batch Gradient Descent，BGD)**

1. 将$J(\theta)$ 对$\theta$ 求偏导，得到每个$\theta$ 对应的梯度
   $$
   \frac{\partial{J(\theta)}}{\partial{\theta_j}} =  - \frac{1}{m}\sum_{i=1}^{m}(y_i - h_{\theta}(x^i))x_j^i
   $$

2. 需要最小化损失函数，所以按每个参数$\theta$ 的`梯度负方向`，来更新每个$\theta$ `(问题： 为什么梯度方向变化值为下一个要更新的值！如何计算！)`
   $$
   \theta_j' = \theta_j +\frac{1}{m}\sum_{i=1}^{m}(y_i - h_{\theta}(x^i))x_j^i
   $$

3. 从上面公式可以注意到，它得到的是一个全局最优解，但是每迭代一步，都要用到训练集所有的数据，如果m很大，那么可想而知这种方法的迭代速度会相当的慢。所以，这就引入了另外一种方法——随机梯度下降。

　对于批量梯度下降法，样本个数m，x为n维向量，一次迭代需要把m个样本全部带入计算，迭代一次计算量为$m * n^2$

**(2). 随机梯度下降法(Random Gradient Descent，RGD)** 

1. 将损失函数写成如下形式，则其中对应的是训练集中每个样本的粒度
   $$
   J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \frac{1}{2}(y^i - h_{\theta}(x^i))^2 = \frac{1}{m} \sum_{i=1}^{m} cos \ t(\theta, (x^i, y^i)) \\
   cos \ t(\theta, (x^i, y^i)) = \frac{1}{2}(y^i - h_{\theta}(x^i))^2
   $$

2. 对$\theta$ 求偏导，得到对应的梯度用于更新$\theta$ 
   $$
   \theta_j' = \theta_j +(y^i - h_{\theta}(x^i))x_j^i
   $$
   随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将theta迭代到最优解了，对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。但是，RGD伴随的一个问题是噪音较BGD要多，使得RGD并不是每次迭代都向着整体最优化方向。

3. 随机梯度下降每次迭代只使用一个样本，迭代一次计算量为$n^2$，当样本个数m很大的时候，随机梯度下降迭代一次的速度要远高于批量梯度下降方法。两者的关系可以这样理解：随机梯度下降方法以损失很小的一部分精确度和增加一定数量的迭代次数为代价，换取了总体的优化效率的提升。增加的迭代次数远远小于样本的数量。

`总结`

批量梯度下降---最小化所有训练样本的损失函数，使得最终求解的是全局的最优解，即求解的参数是使得风险函数最小，但是对于大规模样本问题效率低下。

随机梯度下降---最小化每条样本的损失函数，虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近，适用于大规模训练样本情况。

###3.2 牛顿法和拟牛顿法（Newton's method & Quasi-Newton Methods）

**(1) 牛顿法（Newton's method）**

牛顿法是一种在实数域和复数域上近似求解方程的方法。方法使用函数f (x)的泰勒级数的前面几项来寻找方程f (x) = 0的根。牛顿法最大的特点就在于它的收敛速度很快。

首先，选择一个接近函数 f (x)零点的$x_0$ ，计算相应的$f(x_0)$ 和切线斜率$f'(x_0)$ ，然后计算穿过点$(x_0, \ f(x_0))$ 并且斜率为$f'(x_0)$ 的直线与x轴交点的x坐标，即求解方程
$$
x \cdot f'(x_0) + f(x_0) - x_0 \cdot f'(x_0) = 0
$$
将新求得的x坐标命名为$x_1$ ，通常$x_1$ 会比$x_0$ 更接近方程$f(x)=0$的解，然后就可以用$x_1$ 进行下一轮的迭代，迭代公式如下所示
$$
x_{n+1} = x_n + \frac{f(x_n)}{f'(x_n)}
$$
已证明，如果$f'(x)$ 是连续的并且待求的`零点x孤立`的，那么在零点周围存在一个区域，只要初始值$x_0$ 位于这个邻近区域内，那么牛顿法必定`收敛`。若$f'(x) \ne 0$ ，那么牛顿法将具有平方收敛性能，即每迭代一次，牛顿结果的有效数字增加一倍。由于牛顿法是基于当前位置的切线来确定下一次的位置，所以牛顿法又被很形象地称为是"切线法"。牛顿法的搜索路径（二维情况）如下图所示

![NewtonIteration_Ani](E:/SoftwareForWin10/ML基础/img/md_imgs/NewtonIteration_Ani.gif)

`牛顿法与梯度下降效率对比` 

从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。如果更通俗地说的话，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。（牛顿法目光更加长远，所以少走弯路；相对而言，梯度下降法只考虑了局部的最优，没有全局思想。）

　　根据wiki上的解释，从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。其中红色为牛顿法的迭代路径，绿色为梯度下降的迭代路径。

![newton_vs_geadient_decent](E:/SoftwareForWin10/ML基础/img/md_imgs/newton_vs_geadient_decent.png)

`牛顿法的优缺点总结`

- 优点：二阶收敛，收敛速度快；
- 缺点：牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。

上述（1）中的牛顿法采用求根方式得到，对于最优化问题其极值点处有一个特性：在极值点处函数的一阶导数为0，因此可以在一阶导数处利用牛顿法通过迭代方式求最优解（求一阶导数对应函数的根），我们也可以通过函数在$x_k$ 点进行二阶泰勒展开
$$
f(x) = f(x_k) + f'(x_k)(x - x_k) + \frac{1}{2}f''(x_k)(x - x_k)^2 \\
\downarrow \\
\frac{f(x) - f(x_k)}{x - x_k} = f'(x_k) + f''(x_k)(x - x_k)
\downarrow  \\
当x  \rightarrow x_k时，f'(x) = f'(x_k) + f''(x_k)(x - x_k)，假定点x_{k+1}是一阶导数的根，则有 \\
f'(x_k + 1) = f'(x_k) + f''(x_k)(x_{k+1} - x_k) = 0 \\
\downarrow \\
x_{k+1} = x_k - \frac{f'(x_k)}{f''(x_k)}
$$
因此我们得到了一个不断更新x迭代求得最优解的方法。以上是针对单变量求得的结果，对多变量就要引入雅克比矩阵（[Jacobian](http://jacoxu.com/?p=146)）和海森矩阵（[Hessian](http://jacoxu.com/?p=146)），雅克比矩阵为函数对各自变量的一阶导数，海森矩阵为函数对自变量的二次微分。
$$
令x = (x_1, x_2, ..., x_n)多元变量构成行向量，则f(x)为对应的函数值，则对f一阶求导 \\
雅克比矩阵 \ J_f(x_1, x_2, ..., x_n) = \frac{\partial (y_1, y_2, ..., y_m) }{\partial (x_1, x_2, ..., x_n) } = 
\left[ \begin{matrix}
\frac{\partial y_1}{\partial x_1} & \frac{\partial y_1}{\partial x_2} & ... & \frac{\partial y_1}{\partial x_n} \\
\frac{\partial y_2}{\partial x_1} & \frac{\partial y_2}{\partial x_2} & ... & \frac{\partial y_2}{\partial x_n} \\
... & ... & ... & ... \\
\frac{\partial y_m}{\partial x_1} & \frac{\partial y_m}{\partial x_2} & ... & \frac{\partial y_m}{\partial x_n} \\
\end{matrix}\right] \\
\quad \\
海森矩阵 \  \ H(x_1, x_2, ..., x_n) = \frac{\partial f }{\partial x } = 
\left[ \begin{matrix}
\frac{\partial^2 f}{\partial^2 x_1} & \frac{\partial^2 f}{\partial x_1 \partial x_2 } & ... & \frac{\partial f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial^2 x_2 } & ... & \frac{\partial f}{\partial x_2 \partial x_n} \\
... & ... & ... & ... \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2 } & ... & \frac{\partial^2 f}{ \partial^2 x_n} \\
\end{matrix}\right] \\
$$

$$
例1： y_1 = x_1,  \ y_2=5x_3, \ y_3 = 4x_2^2 - 2x_3, \  y_4 = x_3sin(x_1), 则对应的雅克比矩阵为 \\
J_f(x_1, x_2, x_3) =
\left[ \begin{matrix}
\frac{\partial y_1}{\partial x_1} & \frac{\partial y_1}{\partial x_2} &\frac{\partial y_1}{\partial x_3}  \\
\frac{\partial y_2}{\partial x_1} & \frac{\partial y_2}{\partial x_2} &\frac{\partial y_2}{\partial x_3}  \\
\frac{\partial y_3}{\partial x_1} & \frac{\partial y_3}{\partial x_2} &\frac{\partial y_3}{\partial x_3}  \\
\frac{\partial y_4}{\partial x_1} & \frac{\partial y_4}{\partial x_2} &\frac{\partial y_4}{\partial x_3} \\
\end{matrix}\right]  = 
\left[ \begin{matrix}
1 & 0&0 \\
0 & 0&5  \\
0 & 8x_2 &-2  \\
x_3 cos(x_1) & 0 &xin(x_2)  \\
\end{matrix}\right] \\
例子2：f(x) = x_1^2 + 2x_1x_2 + 3x_2^2 + 4x_1 + 5x_2 + 6，则对应的海森矩阵为 \\
H(x_1, x_2) = \frac{\partial f }{\partial x } = 
\left[ \begin{matrix}
\frac{\partial^2 f}{\partial^2 x_1} & \frac{\partial^2 f}{\partial x_1 \partial x_2 }  \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial^2 x_2 }  \\
\end{matrix}\right] = 
\left[ \begin{matrix}
2 & 2  \\
2 &  6 \\
\end{matrix}\right] \\
$$

对于高维函数，其一阶导数为雅克比矩阵，记为$J_f(x)$。其二阶导数就变为了一个海森矩阵，记为$H(x)$  ，则牛顿迭代公式就变为
$$
x_{k+1} = x_k - \frac{f'(x_k)}{f''(x_k)} \\
\downarrow \\
x_{k+1} = x_k - H^{-1}_k\cdot g_k ，\quad(令 g_k = J_f(x_k))\\
$$
上述的牛顿法需要计算Hessian矩阵的逆矩阵，运算复杂度太高，在动辄百亿、千亿量级特征的大数据时代，模型训练耗时太久。当$H_{k}$ 为正定时，$H_{k}^{-1}$ 也为正定，可以保证牛顿法搜索的方向是向下搜索。当目标函数是二次函数时，由于二次泰勒展开函数与原目标函数不是近似而是完全相同的二次式，海森矩阵就退化成一个常数矩阵，从任意点出发，只需一步迭代就可达到f(x)的极小值。因此牛顿法具有`二次收敛性` ；对于非二次函数，若函数的二次性态较强，或迭代点金进入极小点邻域，则其收敛速度也是很快。

原始牛顿法由于迭代公式中没有步长因子，而是定步长迭代，对于非二次型目标函数，会使函数值上升出现$f(x_{k+1})  \gt  f(x_k)$ 的情况，说明牛顿法不能保证函数值稳定下降，严重情况下会因为迭代点序列$\{x_k\}$ 发散而导致计算失败。

**阻尼牛顿法** 

阻尼牛顿法每次迭代的方向仍采用牛顿方向 $d_k = - H^{-1}_k \cdot g_k$ ，但是每次迭代需沿此方向做一维搜索（linear search），需要最优步长因子$\lambda_k$ 
$$
\lambda_k = arg \  min_{\lambda \in R} \ f(x_k + \lambda \ d_k)
$$
`阻尼牛顿算法`

1. 给定初始值$x_0$ 和进度阈值$\epsilon$ ，并令k=0
2. 计算$g_k$ 和$H_k$
3. 若$||g_k|| \lt \epsilon$ ，则停止迭代；否则确定搜索方向$d_k = -H^{-1}_k \cdot g_k$
4. 利用最优步长因子$\lambda_k$ ，得到$x_{k+1} = x_k + \lambda_k \ d_k$
5. 令k=k+1，转至步2

**(2) 拟牛顿法（Quasi-Newton Methods）**

拟牛顿法是`求解非线性优化问题`最有效的方法之一。拟牛顿法的本质思想不用二阶偏导数而构造出可以近似海森矩阵逆的正定对称阵，在`拟牛顿条件`下优化目标函数。从而简化了运算的复杂度。

拟牛顿法和最速下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于最速下降法，尤其对于困难的问题。另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。如今，优化软件中包含了大量的拟牛顿算法用来解决无约束，约束，和大规模的优化问题。

特点：1. 只需用到函数的一阶梯度，而Newton法用到二阶Hessian阵；2. 下降算法，故全局收敛；3. 不需求矩阵逆，计算量小；4. 一般可达到超线性收敛，速度快；5. 有二次终结性。

。常用的拟牛顿法有DFP算法和BFGS算法

**拟牛顿条件/方程（割线方程）** 

提出用来近似的矩阵应该满足的条件，为其提供理论指导。下面用B表示对海森矩阵H本身的近似，而用D表示对海森矩阵的逆$H^{-1}$ 的近似，满足$B \approx H, \ D \approx H^{-1}$ 。设经过k+1次迭代后得到$x_{k+1}$ ,此时将目标函数f(x)在$x_{k+1}$ 附近的泰勒展开，取二阶近似得到
$$
f(x) \approx f(x_{k+1}) + \triangledown f(x_{k+1}) \cdot (x - x_{k+1}) + \frac{1}{2} \cdot (x - x_{k+1})^T \cdot \triangledown^2 f(x_{k+1}) \cdot (x - x_{k+1}) \\
 \triangledown f(x) \approx\ \triangledown  f(x_{k+1}) + H_{k+1} \cdot (x -x_{k+1})    \quad (两边同时作用梯度算子 \triangledown) \\
  g_{k+1} - g_k \approx H_{k+1} \cdot (x_{k+1} - x_k)   \quad (取x = x_k，并整理) \\
 令 s_k = x_{k+1} - x_k， \ y_k = g_{k+1} - g_k ，代入可以得到 \\
 \downarrow \\
 y_k \approx H_{k+1} \cdot s_k \quad 或  \quad s_k \approx H^{-1}_{k+1} \cdot y_k   \tag{1}\\
 (拟牛顿条件)
$$
**DFP(Davidon-Fletcher-Powell)**

算法核心是通过迭代方法对$H^{-1}_{k+1}$ 做近似，迭代公式为
$$
D_{k+1} = D_k + \Delta D_k, k=0,1,2,...，其中D_0=I 且  \Delta D_k为正交阵 \tag{2}
$$
其中$\Delta D_k$ 为待定矩阵，假设待定矩阵的形式为
$$
\Delta D_k = \alpha \ u \ u^T + \beta \ v \ v^t \tag{3}
$$
其中$\alpha，\beta$ 为待定系数，$u, \ v \in R^N$ 为待定向量，从形式上看，这种待定公式保证了$\Delta D_k$ `对称性` 。将（3）代入（2）并结合（1）式可以得到
$$
s_k = D_{k+1}  \cdot y_k = (D_k + \alpha \ u\ u^T + \beta \ v \ v^t) \cdot y_k = D_k \ y_k + \alpha \ u \ u^T \ y_k + \beta \ v  \ v^T \ y_k \\
= D_k \ y_k + (\alpha \ u^T \ y_k) \ u + (\beta \ v^T \ y_k) \ v  \ ,  (括号中的两个都是数)  \\
\quad \\
令 \alpha \ u^T \ y_k =1， \quad \beta \ v^T \ y_k = -1 可以得到(u, v为待定)：\\ 
\alpha = \frac{1}{u^T \ y_k}，\beta = - \frac{1}{v^T \ y_k}，代入上式s_k得到：
\quad \\
u - v  =s_k - D_k \ y_k，则可直接令u = s_k , \  v=D_k \ y_k，则有 \\
\quad \\
\alpha = \frac{1}{s_k^T \ y_k}，\beta = - \frac{1}{(D_k \ y_k)^T \ y_k} = -\frac{1}{ y_k^T \ D_k \ y_k} \tag{4}
$$
将（4）是计算出来的$\alpha， \beta$ 结果代入（3）式就可以得到校正矩阵$\Delta D_k$ 
$$
\Delta D_k =  \frac{s_k \ s_k^T}{s_k^T \ y_k} -\frac{D_k \ y_k  \ y_k^T \ D_k}{ y_k^T \ D_k \ y_k} \\
\quad \\
则D_k 的修正公式为：D_{k+1} = D_k +  \frac{s_k \ s_k^T}{s_k^T \ y_k} -\frac{D_k \ y_k  \ y_k^T \ D_k}{ y_k^T \ D_k \ y_k}
$$
DEP算法步骤

1. 给定初始值$x_0$ 和进度阈值$\epsilon$ ，并令$D_0 = I, k=0$ ；
2. 确定搜索方向为$d_k = - D_k \cdot g_k$
3. 利用阻尼牛顿法得到的步长因子$\lambda_k$ ，令$s_k = \lambda_k \ d_k, \quad x_{k+1} = x_k + s_k$ ；
4. 若$||g_k+1|| \lt \epsilon$ ，则算法结束
5. 计算$y_k = g_{k+1} - g_k$
6. 计算$D_{k+1} = D_k +  \frac{s_k \ s_k^T}{s_k^T \ y_k} -\frac{D_k \ y_k  \ y_k^T \ D_k}{ y_k^T \ D_k \ y_k}$ 
7. 令k = k + 1，转到步骤2

###3.3 共轭梯度法（Conjugate Gradient）

共轭梯度法是介于最速下降法与牛顿法之间的一个方法，它仅需利用一阶导数信息，但克服了最速下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hesse矩阵并求逆的缺点，共轭梯度法不仅是解决大型线性方程组最有用的方法之一，也是解大型非线性最优化最有效的算法之一。 在各种优化算法中，共轭梯度法是非常重要的一种。其优点是所需存储量小，具有步收敛性，稳定性高，而且不需要任何外来参数。

其中绿色为梯度下降法，红色代表共轭梯度法。

![-Conjugate_gradient_illustration](E:/SoftwareForWin10/ML基础/img/md_imgs/-Conjugate_gradient_illustration.png)





###3.4 启发式优化方法

启发式方法指人在解决问题时所采取的一种根据经验规则进行发现的方法。其特点是在解决问题时,利用过去的经验,选择已经行之有效的方法，而不是系统地、以确定的步骤去寻求答案。启发式优化方法种类繁多，包括经典的模拟退火方法、遗传算法、蚁群算法以及粒子群算法等等。

还有一种特殊的优化算法被称之`多目标优化算法`，它主要针对同时优化多个目标（两个及两个以上）的优化问题，这方面比较经典的算法有NSGAII算法、MOEA/D算法以及人工免疫算法等。





http://www.cnblogs.com/maybe2030/p/4665837.html



###3.5 解决约束优化问题——拉格朗日乘数法





http://www.cnblogs.com/maybe2030/p/4946256.html































